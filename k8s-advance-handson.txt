08.01.2021
==========

Pod-recap
=========

vim pod-defintion.yml

apiVersion: v1

kind: Pod

metadata:

  name: myapp-pod
  labels: 
    app: myapp


spec:
  containers:
    - name: nginx-container
      image: nginx
    
    - name: redis-db
      iamge: redis

$ kubectl create -f pod-definition.yml

$ kubectl get pods

$ kubectl describe pod myapp-pod

----------------------------------------------------------------------------------------------- 

16.01.2020
==========

pod practice test in kode cloud
===============================

1. How many pods exist on the system

worker node
-----------

kubectl get pods

No resource found in default namespace

so answer is 0

2. create new pod with nginx image

$ kubectl run nginx --image=nginx

$ kubectl get pods

click check


3. How many created now

$ kubectl get pods

Answer is 4


4. What is the image used to create new pod

$ kubectl describe pod newpods-fj6kk | grep -i image 

Answer is busybox

5. which nodes pod are placed on 

$ kubectl get pods -o wide

Answer is Node01


6. what image used create webapp pod

$ kubectl describe pod webapp  | grep -i image


Answer busybox and nginx


7. what is state of the container agentx in the pod webapp


$ kubect describe pod webapp | grep state

Answer is waiting

8. why do you think container agentx in pod webapp error

$ kubectl describe pod webapp | grep -i error

Ansswer image doesn't exist in docker hub


9. what does ready column output in kubectl get pod command indicate

Answer number of running conatiner on the pod

10. Delete webapp pod

$ kubectl delete pod webapp



11. create new pod with name of redis image redis123

$ kubectl run redis --image=redis123 --dry-run=client -o yaml > pod.yaml

$ kubectl apply -f pod.yaml

12. fix the the image name

$ kubectl edit pod redis


change image redis123 to redis


-------------------------------------------------------------------------------------------------------

24.01.2021
==========

recap-replicaset-practice-test in kodecloud
-------------------------------------------

1. how many pods exist on the system

$ kubectl get pods

No resource found in default namespace

answer is 0


2. how many replicaset exist on the system

$ kubectl get replicasets

no resource found on default namespace

Asnwer is 0

3. how about now? how many replicasets do you see

$ kubectl get replicasets

NAME              DESIRED     CURRENT    READY    AGE
new-replica-set      4           4         0       5s

answer is 1


4. How many pods DESIRED in the new-replicasets

answer is 4

5. what is the image used to create pod in the replicaset


6. what is image used to create pod in the replicaset

$ kubectl describe replicaset new-replica-set | grep -i image

image : busybox 777

answer is busybox777


7. how many pod in ready date in replicaset

kubectl describe replicaset new-replica-set | grep -i running

0 Running

answer is 0

8. why do you think pods are not ready


$ kubectl describe pod new-replica-set-5t6we

error image doesn't exist

answer is busybox777 image not exist

9. delete one of the pods in 4 pods

$ kubectl delete pod new-replica-set-9ty45


10. how many pod exist now

answer is 4

11. why still 4 pods running after delete one pod


answer is Replicaset ensure desire number of pod running at all time

12. create replicaset using defintion file replicaset-defintion-1.yaml


version error

edit file chnage version v1 to apps/v1


$ kubectl apply -f replicaset-defintion-1.yaml

relicaset-1 created

13. create replicaset using defintion file replicaset-defintion-2.yaml


label error error

edit file chnage lable name nginx to frontend 


$ kubectl apply -f replicaset-defintion-2.yaml

relicaset-2 created

14. delete two newly created replicasets 

$ kubectl delete replicaset replicaset-1

$ kubectl delete replicaset replicaset-2



15. fix the original replicaset correct the image


$ kubectl edit replicaset new-replica-set

change image bsuybox777 to busybox

delete the older pods


16. scale the replicaset up to 5replicas

$  kubectl scale replicaset new-replica-set --replicas=5

17. scale down the replicaset up to 2 replicas

$ kubectl scale replicaset new-replica-set --replicas=2



----------------------------------------------------------------------------------------------------------------------------
27.01.2021
==========

Deployment-practice on kodecloude
---------------------------------

1. How many pod exist on the system

$ kubectl get pods

Answer is 0

2. How many replicasets exist on the system

$ kubectl get replicasets

Answer is 0

3. how many deployment exist on the system

$ kubectl get deployment

Answer is 0

4. how many deployment exist on the system

now we created new one

$ kubectl get deployment

Answer is 1

5. how many replicaset exist on the system

$ kubectl get replicaset

Answer is 1

6. how many pods exist now

$ kubectl get pods

Answer is 4

7. out of all existing how many are ready

Answer is 0

8. what image used to create pod in the deployment

$ kubectl describe deployment  frontend-deployment | grep -i image

Answer is busybox888

9. why do think deployment not ready

$ kubectl decribe pod podname

Answer is image doesn't exist error

10. create deployment new file deployment-definition-1.yaml

error kind: deployment  usong small d


vim deployment-definition-1.yaml

change deployment to Deployment

$ kubectl apply -f deployment-definition-1.yaml



11. create own deployment using below attribute

name= httpd-frontend image= httpd:2.4-alpine --replicas=3

$ kubect create deployment httpd-frontend --image=httpd:2.4-alpine --replicas=3

---------------------------------------------------------------------------------------------------------------------------

28.01.2021
==========

namespace-practice-tests
------------------------

1. How many Namespaces exist on the system?


Remember to use Google Chrome to open this quiz portal. It may hang going forward.

$ kubectl get ns

NAME              STATUS   AGE
default           Active   111s
dev               Active   35s
finance           Active   35s
kube-node-lease   Active   113s
kube-public       Active   113s
kube-system       Active   113s
manufacturing     Active   35s
marketing         Active   35s
prod              Active   35s
research          Active   35s


kubectl get ns --no-headers | wc -l
10

Answer is 10



2. How many pods exist in the 'research' namespace?


kubectl get pods -n research
NAME    READY   STATUS             RESTARTS   AGE
dna-1   0/1     CrashLoopBackOff   5          4m46s
dna-2   0/1     CrashLoopBackOff   5          4m46s


Answer is 2


3.Create a POD in the 'finance' namespace.


Use the spec given on the right.


Name: redis
Image Name: redis



kubectl run redis --image=redis --dry-run=client -o yaml > pod.yaml

vim pod.yaml

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: redis
  name: redis
  namespace: finance
spec:
  containers:
  - image: redis
    name: redis
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}

add namspace in metadata section


kubectl apply -f pod.yaml 
pod/redis created

$ kubectl get pods -n finance 
NAME      READY   STATUS    RESTARTS   AGE
payroll   1/1     Running   0          9m14s


4. Which namespace has the 'blue' pod in it?

kubectl get pods --all-namespaces | grep blue
marketing       blue                                   1/1     Running            0          13m

Answer is marketing


5. Access the Blue web application using the link above your terminal


From the UI you can ping other services



6. What DNS name should the Blue application use to access the database 'db-service' in its own namespace - 'marketing'.


You can try it in the web application UI. Use port 3306.




Blue - Marketing Application
Connectivity Test
Host Name  db-service
Host Port  3306


test

Answer is db-service



7. What DNS name should the Blue application use to access the database 'db-service' in the 'dev' namespace


You can try it in the web application UI. Use port 3306.


Answer db-service.dev.svc.cluster.local


------------------------------------------------------------------------------------------------------

31.01.2021
==========

services-pracice-test kodekloud
-------------------------------

1. How many Services exist on the system?


in the current(default) namespace


$ kubectl get svc

NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   47m


Ans: 1


2.What is the type of the default 'kubernetes' service?



$ kubectl get svc

NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   47m


Ans: ClusterIp


3. What is the 'targetPort' configured on the 'kubernetes' service?

$ kubectl describe svc kubernetes 

Name:              kubernetes
Namespace:         default
Labels:            component=apiserver
                   provider=kubernetes
Annotations:       <none>
Selector:          <none>
Type:              ClusterIP
IP:                10.96.0.1
Port:              https  443/TCP
TargetPort:        6443/TCP
Endpoints:         172.17.0.32:6443
Session Affinity:  None
Events:            <none>


Ans: 6443

4. How many labels are configured on the 'kubernetes' service?

Labels:            component=apiserver
                   provider=kubernetes

Ans: 2


5. How many Endpoints are attached on the 'kubernetes' service?

Endpoints:         172.17.0.32:6443

Ans: 1


6. How many Deployments exist on the system now?


in the current(default) namespace


$ kubectl get deployments.apps 
NAME                       READY   UP-TO-DATE   AVAILABLE   AGE
simple-webapp-deployment   4/4     4            4           67s


Ans: 1



7. What is the image used to create the pods in the deployment?


$ kubectl describe deployments.apps simple-webapp-deployment | grep -i image

    Image:        kodekloud/simple-webapp:red

Ans:  kodekloud/simple-webapp:red


8. Are you able to accesss the Web App UI?


Try to access the Web Application UI using the tab simple-webapp-ui above the terminal.


not deployed yet 

Ans: No


9. Create a new service to access the web application using the service-definition-1.yaml file


Name: webapp-service; Type: NodePort; targetPort: 8080; port: 8080; nodePort: 30080; selector: simple-webapp

$ kubectl expose deployment simple-webapp-deployment --name=webapp-service --type=NodePort --port=8080 --dry-run=client -o yaml > svc.yaml


vim svc.yaml

apiVersion: v1
kind: Service
metadata:
  creationTimestamp: null
  name: webapp-service
spec:
  ports:
  - port: 8080
    protocol: TCP
    targetPort: 8080
    nodePort: 30080
  selector:
    name: simple-webapp
  type: NodePort
status:
  loadBalancer: {}

controlplane $ vim svc.yaml 




$ kubectl apply -f svc.yaml

----------------------------------------------------------------------------------------------------------------------

03.02.2021
==========


imperative-solution-in-kodekloud
--------------------------------


1. Deploy a pod named nginx-pod using the nginx:alpine image.


Use imperative commands only.


$ kubectl run nginx-pod --image=nginx:alpine 

pod/nginx-pod created

$ kubectl describe pod nginx-pod


2. Deploy a redis pod using the redis:alpine image with the labels set to tier=db.


Either use imperative commands to create the pod with the labels. Or else use imperative commands to generate the pod definition file, then add the labels before creating the pod using the file.



Pod Name: redis
Image: redis:alpine
Labels: tier=db



$ kubectl run redis --image=redis:alpine --labels=tier=db 
pod/redis created


$ kubectl get pod redis
NAME    READY   STATUS    RESTARTS   AGE
redis   1/1     Running   0          10s



4. Create a service redis-service to expose the redis application within the cluster on port 6379.


Use imperative commands


Service: redis-service
Port: 6379
Type: ClusterIp
Selector: tier=db


$ kubectl expose pod redis --name redis-service --port 6379 --target-port 6379
service/redis-service exposed

$ kubectl get svc redis-service 

NAME            TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE
redis-service   ClusterIP   10.96.194.35   <none>        6379/TCP   9s


$ kubectl describe svc redis-service




5. Create a deployment named webapp using the image kodekloud/webapp-color with 3 replicas


Try to use imperative commands only. Do not create definition files.


kubectl create deployment webapp --image kodekloud/webapp-color --dry-run=client -o yaml > dep.yaml

 $ vim dep.yaml  

change replicas 1 to 3


 $ kubectl apply -f dep.yaml 

deployment.apps/webapp created


$ kubectl get deployments.apps webapp 
NAME     READY   UP-TO-DATE   AVAILABLE   AGE
webapp   1/1     1            1           11s



6. Create a new pod called custom-nginx using the nginx image and expose it on container port 8080


kubectl run custom-nginx --image nginx --port 8080 --expose --dry-run=client -o yaml


 $ kubectl run custom-nginx --image nginx --port 8080 --expose 


$ kubectl get svc custom-nginx 

NAME           TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
custom-nginx   ClusterIP   10.103.199.104   <none>        8080/TCP   28s



$ kubectl scale deployment webapp --replicas=3
deployment.apps/webapp scaled



7. Create a new namespace called dev-ns.


Use imperative commands.


$ kubectl create ns dev-ns 
namespace/dev-ns created




8.Create a new deployment called redis-deploy in the dev-ns namespace with the redis image. It should have 2 replicas.


Use imperative commands.



$ kubectl create deployment redis-deploy --image redis --namespace dev-ns --replicas 2
deployment.apps/redis-deploy created

$ kubectl get deployments.apps redis-deploy -n dev-ns
NAME           READY   UP-TO-DATE   AVAILABLE   AGE
redis-deploy   2/2     2            2           82s



9. Create a pod called httpd using the image httpd:alpine in the default namespace. Next, create a service of type ClusterIP by the same name (httpd). The target port for the service should be 80.


Try to do this with as few steps as possible.



`httpd` pod created with the correct image?
'httpd' service is of type 'clusterIP'?
'httpd' service uses correct target port 80?
'httpd' service exposes the 


$ kubectl run httpd --image httpd:alpine --port 80 --expose 
service/httpd created
pod/httpd created

------------------------------------------------------------------------------------------------------------------------
04.02.2021
===========

Manual-scheduling-solution-in-kodekloud
---------------------------------------


1. A pod definition file nginx.yaml is given. Create a pod using the file.


Only create the POD for now. We will inspect its status next.


$ kubectl apply -f nginx.yaml 
pod/nginx created


$ kubectl get pods
NAME    READY   STATUS    RESTARTS   AGE
nginx   0/1     Pending   0          29s



2. What is the status of the created POD?


$ kubectl get pods
NAME    READY   STATUS    RESTARTS   AGE
nginx   0/1     Pending   0

Ans: pending 


3. Why is the POD in a pending state?


Inspect the environment for various kubernetes control plane components.


 $ kubectl get pods -n kube-system
NAME                                   READY   STATUS    RESTARTS   AGE
coredns-f9fd979d6-8pcsb                1/1     Running   0          8m14s
coredns-f9fd979d6-dv7q8                1/1     Running   0          8m14s
etcd-controlplane                      1/1     Running   0          8m21s
kube-apiserver-controlplane            1/1     Running   0          8m21s
kube-controller-manager-controlplane   1/1     Running   0          8m21s
kube-flannel-ds-amd64-h69kf            1/1     Running   0          8m4s
kube-flannel-ds-amd64-r5s7d            1/1     Running   0          8m14s
kube-proxy-6ngk4                       1/1     Running   0          8m4s
kube-proxy-c6vgh                   


Ans: no scheduler present



4. Manually schedule the pod on node01


Delete and re-create the POD if necessar

 $ kubectl delete pod nginx
pod "nginx" deleted 

vim nginx.yaml

. add nodename in spec section

apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  -  image: nginx
     name: nginx
  nodeName: node01
~                     


$ kubectl apply -f nginx.yaml 
pod/nginx created

$ kubectl get pods
NAME    READY   STATUS    RESTARTS   AGE
nginx   1/1     Running   0          18s



5. Now schedule the same pod on the master/controplane node.


Delete and recreate the POD if necessary


$ kubectl delete pod nginx

pod "nginx" deleted


vim nginx.yaml


. add nodename in spec section

apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  -  image: nginx
     name: nginx
  nodeName: master

$ kubectl apply -f nginx.yaml

--------------------------------------------------------------------------------------------------------


10.02.2021
==========

practice-test-for-labels-and-selector-kodekloud
-----------------------------------------------

1. We have deployed a number of PODs. They are labelled with 'tier', 'env' and 'bu'. How many PODs exist in the 'dev' environment?


Use selectors to filter the output



$ kubectl get pods --show-labels
NAME          READY   STATUS    RESTARTS   AGE   LABELS
app-1-4ml4z   1/1     Running   0          55s   bu=finance,env=dev,tier=frontend
app-1-766k9   1/1     Running   0          55s   bu=finance,env=dev,tier=frontend
app-1-dznr5   1/1     Running   0          55s   bu=finance,env=dev,tier=frontend
app-1-zzxdf   1/1     Running   0          55s   bu=finance,env=prod,tier=frontend
app-2-2vdql   1/1     Running   0          55s   env=prod,tier=frontend
auth          1/1     Running   0          55s   bu=finance,env=prod
db-1-bcck9    1/1     Running   0          55s   env=dev,tier=db
db-1-lnlxd    1/1     Running   0          55s   env=dev,tier=db
db-1-spghz    1/1     Running   0          55s   env=dev,tier=db
db-1-vjwq6    1/1     Running   0          55s   env=dev,tier=db
db-2-5fwqk    1/1     Running   0          55s   bu=finance,env=prod,tier=db


$ kubectl get pods -l env=dev --no-headers | wc -l
7


2. How many PODs are in the 'finance' business unit ('bu')?


$ kubectl get pods -l bu=finance --no-headers | wc -l
6


3. How many objects are in the 'prod' environment including PODs, ReplicaSets and any other objects?

kubectl get all -l env=prod --no-headers | wc -l
7


4. Identify the POD which is part of the prod environment, the finance BU and of frontend tier?


$ kubectl get pods -l env=prod,bu=finance,tier=frontend | awk '{print $1}'

NAME
app-1-zzxdf


5. A ReplicaSet definition file is given 'replicaset-definition-1.yaml'. Try to create the replicaset. There is an issue with the file. Try to fix it.


$ kubectl apply -f replicaset-definition-1.yaml 

The ReplicaSet "replicaset-1" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"tier":"nginx"}: `selector` does not match template `labels`

vim replicaset-definition-1.yaml

change tier nginx to frontend


kubectl apply -f replicaset-definition-1.yaml 
replicaset.apps/replicaset-1 created


----------------------------------------------------------------------------------------------------------------------------
14.02.2021
==========

solution-taints-and-tolerations-kodekloud
-----------------------------------------

1.How many Nodes exist on the system?


including the master/controlplane node


$ kubectl get nodes 
NAME           STATUS   ROLES    AGE   VERSION
controlplane   Ready    master   40m   v1.19.0
node01         Ready    <none>   40m   v1.19.0


$ kubectl get nodes --no-headers  | wc -l
2


2. Do any taints exist on node01?


$ kubectl describe node node01 | grep -i taint
Taints:             <none>
controlplane $ 



3. Create a taint on node01 with key of 'spray', value of 'mortein' and effect of 'NoSchedule'


$ kubectl taint node node01 spray=mortein:NoSchedule
node/node01 tainted


 $ kubectl describe node node01 | grep -i taint
Taints:             spray=mortein:NoSchedule


4. Create a new pod with the NGINX image, and Pod name as 'mosquito'

$ kubectl run mosquito --image=nginx
pod/mosquito created


$ kubectl get pod mosquito
NAME       READY   STATUS    RESTARTS   AGE
mosquito   0/1     Pending   0          33s
controlplane $ 


5. What is the state of the POD?


kubectl get pod mosquito | awk '{print $3}'
STATUS
Pending


6. Why do you think the pod is in a pending state?


Warning  FailedScheduling  64s (x4 over 3m55s)  default-scheduler  0/2 nodes are available: 1 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate, 1 node(s) had taint {spray: mortein}, that the pod didn't tolerate.



7. Create another pod named 'bee' with the NGINX image, which has a toleration set to the taint Mortein


Image name: nginx
Key: spray
Value: mortein
Effect: NoSchedule
Status: Running

$ kubectl run bee --image=nginx --restart=Never --dry-run=client -o yaml > bee.yaml

$ kubectl explain pod --recursive | grep -A5 tolerations
      tolerations       <[]Object>
         effect <string>
         key    <string>
         operator       <string>
         tolerationSeconds      <integer>
         value  <string>

Add below line bee.yaml file under spec

vim bee.yaml

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: bee
  name: bee
spec:
  containers:
  - image: nginx
    name: bee
  tolerations:
  - effect: NoSchedule
    key:    spray
    operator: Equal
    value:  mortein
~                

$ kubectl apply -f bee.yaml 
pod/bee created

$ kubectl get pod bee
NAME   READY   STATUS    RESTARTS   AGE
bee    1/1     Running   0          39s


8. Notice the 'bee' pod was scheduled on node node01 despite the taint.


$ kubectl get pod bee -o wide | awk '{print $7}'
NODE
node01


9. Do you see any taints on master/controlplane node?



$ kubectl describe node controlplane | grep -i taint
Taints:             node-role.kubernetes.io/master:NoSchedule


10. Remove the taint on master/controlplane, which currently has the taint effect of NoSchedule



Node name: master/controlplane


$ kubectl taint node controlplane node-role.kubernetes.io/master:NoSchedule-
node/controlplane untainted


11. What is the state of the pod 'mosquito' now?


$ kubectl get pod mosquito | awk '{print $3}'
STATUS
Running


12. Which node is the POD 'mosquito' on now?


kubectl get pod mosquito -o wide
NAME       READY   STATUS    RESTARTS   AGE   IP           NODE           NOMINATED NODE   READINESS GATES
mosquito   1/1     Running   0          24m   10.244.0.4   controlplane   <none>           <none>


--------------------------------------------------------------------------------------------------------------------

15.02.2021
==========

Practice-test-node-affinity
---------------------------

1. How many Labels exist on node node01?


$ kubectl get nodes node01 --show-labels
NAME     STATUS   ROLES    AGE     VERSION   LABELS
node01   Ready    <none>   2m53s   v1.19.0   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=node01,kubernetes.io/os=linux


Ans: 5 

2. What is the value set to the label beta.kubernetes.io/arch on node01?


beta.kubernetes.io/arch=amd64


Ans: amd64


3. Apply a label color=blue to node node01


$ kubectl label node node01 color=blue



4. Create a new deployment named blue with the nginx image and 6 replicas

$ kubectl create deployment blue --image=nginx 
deployment.apps/blue created


$ kubectl scale deployment blue --replicas=6
deployment.apps/blue scaled


5. Which nodes can the pods for the blue deployment placed on?


 $ kubectl get pods -o wide 
NAME                    READY   STATUS    RESTARTS   AGE     IP           NODE           NOMINATED NODE   READINESS GATES
blue-7bb46df96d-b25g4   1/1     Running   0          4m33s   10.244.0.5   controlplane   <none>           <none>
blue-7bb46df96d-bh8vl   1/1     Running   0          4m33s   10.244.1.4   node01         <none>           <none>
blue-7bb46df96d-jmdgv   1/1     Running   0          4m33s   10.244.1.3   node01         <none>           <none>
blue-7bb46df96d-pxvl6   1/1     Running   0          4m33s   10.244.0.4   controlplane   <none>           <none>
blue-7bb46df96d-w6vw4   1/1     Running   0          4m54s   10.244.1.2   node01         <none>           <none>
blue-7bb46df96d-z8588   1/1     Running   0          4m33s   10.244.1.5   node01         <none>           <none>


Ans: controlplane/node01



6. Set Node Affinity to the deployment to place the pods on node01 only


Name: blue
Replicas: 6
Image: nginx
NodeAffinity: requiredDuringSchedulingIgnoredDuringExecution
Key: color
values: blue


$ kubectl get deployments.apps blue -o yaml > blue.yaml

vim blue.yaml

add below lines under spec section chnage ke=color and value=blue

affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: disktype
            operator: In
            values:
            - ssd       



$ kubectl apply -f blue.yaml 
Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply
deployment.apps/blue configured


7. Which nodes are the pods placed on now?

 kubectl get pods -o wide
NAME                    READY   STATUS    RESTARTS   AGE   IP            NODE     NOMINATED NODE   READINESS GATES
blue-566c768bd6-2mncf   1/1     Running   0          90s   10.244.1.11   node01   <none>           <none>
blue-566c768bd6-8b4z5   1/1     Running   0          94s   10.244.1.8    node01   <none>           <none>
blue-566c768bd6-bltrb   1/1     Running   0          91s   10.244.1.10   node01   <none>           <none>
blue-566c768bd6-gz5nm   1/1     Running   0          94s   10.244.1.7    node01   <none>           <none>
blue-566c768bd6-l5k6c   1/1     Running   0          94s   10.244.1.6    node01   <none>           <none>
blue-566c768bd6-tjbz2   1/1     Running   0          91s   10.244.1.9    node01   <none>           <none>



Ans : node01


8. Create a new deployment named red with the nginx image and 3 replicas, and ensure it gets placed on the master/controlplane node only.


Use the label - node-role.kubernetes.io/master - set on the master/controlplane node.



Name: red
Replicas: 3
Image: nginx
NodeAffinity: requiredDuringSchedulingIgnoredDuringExecution
Key: node-role.kubernetes.io/master
Use the right operator


$ kubectl create deployment red --image=nginx --dry-run=client --replicas=3 -o yaml > red.yaml


vim red.yaml

add below line under spec section

affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: node-role.kubernetes.io/master
                operator: Exists


$  kubectl apply -f red.yaml 
deployment.apps/red created

-----------------------------------------------------------------------------------------------------------------

18.02.2021
==========

Resource limits practice test on kodekloud
------------------------------------------


1.A pod named 'rabbit' is deployed. Identify the CPU requirements set on the Pod

in the current(default) namespace


$ kubectl describe pod rabbit

 Limits:
      cpu:  2
    Requests:
      cpu:        1

Ans: 1


2.Delete the 'rabbit' Pod.


Once deleted, wait for the pod to fully terminate.

$ kubectl delete pod rabbit 
pod "rabbit" deleted


3.Inspect the pod elephant and identify the status.

$ kubectl describe pod elephant | grep -i Reason
      Reason:       CrashLoopBackOff
      Reason:       OOMKilled




4. The elephant runs a process that consume 15Mi of memory. Increase the limit of the elephant pod to 20Mi.


Delete and recreate the pod if required. Do not modify anything other than the required fields.


Pod Name: elephant
Image Name: polinux/stress
Memory Limit: 20Mi



$ kubectl get pod elephant -o yaml > pod.yaml

vim pod.yaml

change memory to 10Mi to 20Mi


$ kubectl delete pod elephant 
pod "elephant" deleted


$ kubectl apply -f pod.yaml 
pod/elephant created



6. Delete the 'elephant' Pod.


Once deleted, wait for the pod to fully terminate.


Delete Pod elephant


$ kubectl delete pod elephant 
pod "elephant" deleted



----------------------------------------------------------------------------------------------------------------------

Daemonset-practice-test-kodekloud
---------------------------------

1. How many DaemonSets are created in the cluster in all namespaces?


Check all namespaces



controlplane $ kubectl get ds --all-namespaces
NAMESPACE     NAME                      DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
kube-system   kube-flannel-ds-amd64     2         2         2       2            2           <none>                   7m34s
kube-system   kube-flannel-ds-arm       0         0         0       0            0           <none>                   7m33s
kube-system   kube-flannel-ds-arm64     0         0         0       0            0           <none>                   7m33s
kube-system   kube-flannel-ds-ppc64le   0         0         0       0            0           <none>                   7m33s
kube-system   kube-flannel-ds-s390x     0         0         0       0            0           <none>                   7m33s
kube-system   kube-proxy                2         2         2       2            2           kubernetes.io/os=linux   7m35s


$ kubectl get ds --all-namespaces --no-headers | wc -l
6

2. Which namespace are the DaemonSets created in?


kube-system


3. which of the below is daemonset


kube-flannel-ds-amd64


4. On how many nodes are the pods scheduled by the DaemonSet kube-proxy


$ kubectl get nodes -n kube-proxy
NAME           STATUS   ROLES    AGE   VERSION
controlplane   Ready    master   13m   v1.19.0
node01         Ready    <none>   12m   v1.19.0


$ kubectl get nodes -n kube-proxy --no-headers | wc -l
2


5. What is the image used by the POD deployed by the kube-flannel-ds-amd64 DaemonSet?


$ kubectl describe ds kube-flannel-ds-amd64 -n kube-system| grep -i image
    Image:      quay.io/coreos/flannel:v0.12.0-amd64


Deploy a DaemonSet for FluentD Logging.


Use the given specifications.


vim elastic.yaml

apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: elasticsearch
  namespace: kube-system
  labels:
    k8s-app: fluentd-logging
spec:
  selector:
    matchLabels:
      name: elasticsearch
  template:
    metadata:
      labels:
        name: elasticsearch
    spec:
      tolerations:
      # this toleration is to have the daemonset runnable on master nodes
      # remove it if your masters can't run pods
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
      containers:
      - name: elasticsearch
        image: quay.io/fluentd_elasticsearch/fluentd:v2.5.2
        resources:
          limits:
            memory: 200Mi
          requests:
            cpu: 100m
            memory: 200Mi
        volumeMounts:
        - name: varlog
          mountPath: /var/log
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
      terminationGracePeriodSeconds: 30
      volumes:
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers
    Image:      quay.io/coreos/flannel:v0.12.0-amd64



kubectl apply -f elastic.yaml 
daemonset.apps/elasticsearch created


-------------------------------------------------------------------------------------------------------

27.02.2021
==========

static-pod-practice-test-kodekloud
----------------------------------

1. How many static pods exist in this cluster in all namespaces?


$ kubectl get pods --all-namespaces | grep "\-control"
kube-system   etcd-controlplane                      1/1     Running   0          5m1s
kube-system   kube-apiserver-controlplane            1/1     Running   0          5m1s
kube-system   kube-controller-manager-controlplane   1/1     Running   0          5m1s
kube-system   kube-scheduler-controlplane            1/1     Running   0          5m1s


$ kubectl get pods --all-namespaces --no-headers | grep "\-control" | wc -l
4


2. Which of the below components is NOT deployed as a static pod?

Ans: coredns



3. Which of the below components is NOT deployed as a static POD?

kube-proxy


4. On what nodes are the static pods created?

$ kubectl get pods --all-namespaces -o wide | grep "\-control"
kube-system   etcd-controlplane                      1/1     Running   0          9m43s   172.17.0.29   controlplane   <none>           <none>
kube-system   kube-apiserver-controlplane            1/1     Running   0          9m43s   172.17.0.29   controlplane   <none>           <none>
kube-system   kube-controller-manager-controlplane   1/1     Running   0          9m43s   172.17.0.29   controlplane   <none>           <none>
kube-system   kube-scheduler-controlplane            1/1     Running   0          9m43s   172.17.0.29   controlplane   <none>           <none>


Ans : controlplane


5. What is the path of the directory holding the static pod definition files?


$ ps -ef | grep kubelet | grep "\--config"
root      2624     1  3 09:02 ?        00:00:22 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml --network-plugin=cni --pod-infra-container-image=k8s.gcr.io/pause:3.2


$ grep -i static /var/lib/kubelet/config.yaml
staticPodPath: /etc/kubernetes/manifests


6. How many pod definition files are present in the manifests folder?

$ ls
etcd.yaml  kube-apiserver.yaml  kube-controller-manager.yaml  kube-scheduler.yaml
controlplane $ ls | wc -l
4
controlplane $ 



7.What is the docker image used to deploy the kube-api server as a static pod?

$ grep -i image kube-apiserver.yaml 

    image: k8s.gcr.io/kube-apiserver:v1.19.0
    imagePullPolicy: IfNotPresent



8. Create a static pod named static-busybox that uses the busybox image and the command sleep 1000


$ kubectl run static-busybox --image=busybox --restart=Never sleep 1000 --dry-run=client -o yaml > static-busybox.yaml

$ kubectl get pods
NAME                          READY   STATUS    RESTARTS   AGE
static-busybox-controlplane   1/1     Running   0          17s


9. Edit the image on the static pod to use busybox:1.28.4

vim static-busybox.yaml

change busybox to busybox:1.28.4


10. We just created a new static pod named static-greenbox. Find it and delete it.

$ kubectl get pods
NAME                          READY   STATUS    RESTARTS   AGE
static-busybox                1/1     Running   0          64s
static-busybox-controlplane   1/1     Running   0          75s
static-greenbox-node01        1/1     Running   0          32s


$ kubectl get node node01 -o wide
NAME     STATUS   ROLES    AGE   VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION       CONTAINER-RUNTIME
node01   Ready    <none>   25m   v1.19.0   172.17.0.32   <none>        Ubuntu 18.04.5 LTS   4.15.0-122-generic   docker://19.3.13


ssh 172.17.0.32

$ ps -ef | grep kubelet | grep "\--config"
root     27686     1  2 09:26 ?        00:00:03 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml --network-plugin=cni --pod-infra-container-image=k8s.gcr.io/pause:3.2


$ grep -i static /var/lib/kubelet/config.yaml

staticPodPath: /etc/just-to-mess-with-you

$ cd /etc/just-to-mess-with-you


 $ rm -rf greenbox.yaml

node01 $ logout
Connection to 172.17.0.32 closed.

-----------------------------------------------------------------------------------------------------------------------------

28.02.2021
==========

multiple scheduler practice test on kodekloud
---------------------------------------------


1. What is the name of the POD that deploys the default kubernetes scheduler in this environment?

$ kubectl -n kube-system get pods | grep "\-scheduler"
kube-scheduler-controlplane            1/1     Running   0          14m



2. What is the image used to deploy the kubernetes scheduler?


Inspect the kubernetes scheduler pod and identify the image



$ kubectl -n kube-system describe pod kube-scheduler-controlplane | grep -i image
    Image:         k8s.gcr.io/kube-scheduler:v1.19.0
    Image ID:      docker-pullable://k8s.gcr.io/kube-scheduler@sha256:529a1566960a5b3024f2c94128e1cbd882ca1804f222ec5de99b25567858ecb9




3. Deploy an additional scheduler to the cluster following the given specification.


Use the manifest file used by kubeadm tool. Use a different port than the one used by the current one.



Namespace: kube-system
Name: my-scheduler
Status: Running
Custom Scheduler Name


cd /etc/kubernetes/manifests/

cp kube-scheduler.yaml /root/my-scheduler.yaml


vim /root/my-scheduler.yaml

        - --leader-elect=false
        - --scheduler-name=my-scheduler


change name to my-scheduler


$ kubectl create -f my-scheduler.yaml


$ kubectl -n kube-system get pods



4. A POD definition file is given. Use it to create a POD with the new custom scheduler.


File is located at /root/nginx-pod.yaml


info_outline
Hint
Name: nginx
Uses custom scheduler
Status: Running


vim nginx-po


under spec section

schedluername: my-scheduler



kubectl create -f nginx-pod.yaml


--------------------------------------------------------------------------------------------------------------------------

28.02.2021
==========

monitoring practice test on kodekloud
-------------------------------------

1. Let us deploy metrics-server to monitor the PODs and Nodes. Pull the git repository for the deployment files.


https://github.com/kodekloudhub/kubernetes-metrics-server.git


$ git clone https://github.com/kodekloudhub/kubernetes-metrics-server.git



2. Deploy the metrics-server by creating all the components downloaded.


Run the 'kubectl create -f .' command from within the downloaded repository.



cd kubernetes-metrics-server/

$ kubectl create -f .


3. It takes a few minutes for the metrics server to start gathering data.


Run the 'kubectl top node' command and wait for a valid output.


$ kubectl top node
NAME           CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   
controlplane   142m         7%     1067Mi          56%       
node01         1998m        99%    588Mi           15%   


4. Identify the node that consumes the most CPU.


node01


5. Identify the node that consumes the most Memory.


Master/controlplane


6. Identify the POD that consumes the most Memory.


$ kubectl top pod
NAME       CPU(cores)   MEMORY(bytes)   
elephant   13m          50Mi            
lion       893m         1Mi             
rabbit     971m         1Mi  

ans: elephant

7. Identify the POD that consumes the most CPU.


Ans: rabbit

----------------------------------------------------------------------------------------------------------------------

28.02.2021
==========

1. We have deployed a POD hosting an application. Inspect it. Wait for it to start.


$ kubectl get pods

NAME       READY   STATUS    RESTARTS   AGE
webapp-1   1/1     Running   0          78s


2. A user - 'USER5' - has expressed concerns accessing the application. Identify the cause of the issue.


Inspect the logs of the POD


$ kubectl logs -f webapp-1 | grep -i user5

[2021-02-28 10:44:54,476] WARNING in event-simulator: USER5 Failed to Login as the account is locked due to MANY FAILED ATTEMPTS.
[2021-02-28 10:44:59,484] WARNING in event-simulator: USER5 Failed to Login as the account is locked due to MANY FAILED ATTEMPTS


3. We have deployed a new POD - 'webapp-2' - hosting an application. Inspect it. Wait for it to start.


$ kubectl get pods
NAME       READY   STATUS    RESTARTS   AGE
webapp-1   1/1     Running   0          4m45s
webapp-2   2/2     Running   0          31s


$ kubectl logs -f webapp-1 | grep -i user5

[2021-02-28 10:47:59,995] WARNING in event-simulator: USER30 Order failed as the item is OUT OF STOCK.


4. A user is reporting issues while trying to purchase an item. Identify the user and the cause of the issue.


Inspect the logs of the webapp in the POD


$ kubectl logs webapp-2  -c simple-webapp | grep -i item

[2021-02-28 10:49:28,148] WARNING in event-simulator: USER30 Order failed as the item is OUT OF STOCK.

--------------------------------------------------------------------------------------------------------------------

04.03.2021
==========

rolling-update-practice-test
----------------------------

1. We have deployed a simple web application. Inspect the PODs and the Services


Wait for the application to fully deploy and view the application using the link above your terminal.


$ kubectl get pods
NAME                        READY   STATUS    RESTARTS   AGE
frontend-7776cb7d57-77pf7   1/1     Running   0          53s
frontend-7776cb7d57-lnxwd   1/1     Running   0          53s
frontend-7776cb7d57-m7xsr   1/1     Running   0          53s
frontend-7776cb7d57-slgfl   1/1     Running   0          53s


2. What is the current color of the web application?

Ans: blue


3. Run the script named curl-test.sh to send multiple requests to test the web application. Take a note of the output.


Execute the script at /root/curl-test.sh.


$ /root/curl-test.sh 

Hello, Application Version: v1 ; Color: blue OK

Hello, Application Version: v1 ; Color: blue OK


4. Inspect the deployment and identify the number of PODs deployed by it


Ans : 4


5. What container image is used to deploy the applications?

$ $ kubectl describe deployments.apps frontend | grep -i "image"
    Image:        kodekloud/webapp-color:v1



5. Inspect the deployment and identify the current strategy

$ kubectl describe deployments.apps frontend | grep -i "strategy"

StrategyType:           RollingUpdate
RollingUpdateStrategy:  25% max unavailable, 25% max surge


6. If you were to upgrade the application now what would happen?


pods are upgraded few at a time




7. Let us try that. Upgrade the application by setting the image on the deployment to 'kodekloud/webapp-color:v2'


Do not delete and re-create the deployment. Only set the new image name for the existing deployment.


$ kubectl edit deployments.apps frontend 
deployment.apps/frontend edited

change v1 to v2



8. Run the script curl-test.sh again. Notice the requests now hit both the old and newer versions. However none of them fail.


Execute the script at /root/curl-test.sh.


$ /root/curl-test.sh 

Hello, Application Version: v2 ; Color: green OK

Hello, Application Version: v2 ; Color: green OK


9. Up to how many PODs can be down for upgrade at a time


Consider the current strategy settings and number of PODs - 4


Ans: 1


10. Change the deployment strategy to 'Recreate'


Do not delete and re-create the deployment. Only update the strategy type for the existing deployment.




10 .Change the deployment strategy to 'Recreate'


Do not delete and re-create the deployment. Only update the strategy type for the existing deployment.


$ kubectl edit deployments.apps frontend 
deployment.apps/frontend edited.

chnage: rollingUpdate to Recreate



11. Upgrade the application by setting the image on the deployment to 'kodekloud/webapp-color:v3'


Do not delete and re-create the deployment. Only set the new image name for the existing deployment.



$ kubectl edit deployments.apps frontend 
deployment.apps/frontend edited.

change v2 to v3 image version



$ kubectl get pods
NAME                        READY   STATUS        RESTARTS   AGE
frontend-7c7fcfc8cb-bh48j   1/1     Terminating   0          10m
frontend-7c7fcfc8cb-cjf44   1/1     Terminating   0          10m
frontend-7c7fcfc8cb-crvqc   1/1     Terminating   0          10m
frontend-7c7fcfc8cb-k9twg   1/1     Terminating   0          10m


12. Run the script curl-test.sh again. Notice the failures. Wait for the new application to be ready. Notice that the requests now do not hit both the versions


Execute the script at /root/curl-test.sh.


$ /root/curl-test.sh 
Hello, Application Version: v3 ; Color: red OK

Hello, Application Version: v3 ; Color: red OK

--------------------------------------------------------------------------------------------------------------------------

06.03.2021
==========

commands-arguments-practice-test-kodekloud
------------------------------------------

1. How many PODs exist on the system?


in the current(default) namespace


$ kubectl get pods
NAME             READY   STATUS    RESTARTS   AGE
ubuntu-sleeper   1/1     Running   0          9m50s


2. What is the command used to run the pod 'ubuntu-sleeper'?

Ans: Command:
      sleep
      4800


3. Create a pod with the ubuntu image to run a container to sleep for 5000 seconds. Modify the file ubuntu-sleeper-2.yaml.


Note: Only make the necessary changes. Do not modify the name.


 vim ubuntu-sleeper-2.yaml 

under container section

command ["sleep", "5000"]
controlplane $ kubectl apply -f ubuntu-sleeper-2.yaml 
pod/ubuntu-sleeper-2 created


4. Create a pod using the file named 'ubuntu-sleeper-3.yaml'. There is something wrong with it. Try to fix it!


Note: Only make the necessary changes. Do not modify the name.



$ vim ubuntu-sleeper-3.yaml 

give sconds with in double quotes

controlplane $ kubectl apply -f ubuntu-sleeper-3.yaml 
pod/ubuntu-sleeper-3 created


5. Update pod 'ubuntu-sleeper-3' to sleep for 2000 seconds.


Note: Only make the necessary changes. Do not modify the name of the pod. Delete and recreate the pod if necessary.




vim ubuntu-sleeper-3.yaml

change time to 2000

 $ kubectl delete pod ubuntu-sleeper-3


$ kubectl apply -f ubuntu-sleeper-3.yaml 
pod/ubuntu-sleeper-3 created


6. Inspect the file 'Dockerfile' given at /root/webapp-color. What command is run at container startup?


 cat Dockerfile
FROM python:3.6-alpine

RUN pip install flask

COPY . /opt/

EXPOSE 8080

WORKDIR /opt

ENTRYPOINT ["python", "app.py"]



7. Inspect the file 'Dockerfile2' given at /root/webapp-color. What command is run at container startup?

cat Dockerfile2

FROM python:3.6-alpine

RUN pip install flask

COPY . /opt/

EXPOSE 8080

WORKDIR /opt

ENTRYPOINT ["python", "app.py"]

CMD ["--color", "red"]controlplane $ 


8. Inspect the two files under directory 'webapp-color-2'. What command is run at container startup?


Assume the image was created from the Dockerfile in this folder

cat webapp-color-pod.yaml 

apiVersion: v1
kind: Pod
metadata:
  name: webapp-green
  labels:
      name: webapp-green
spec:
  containers:
  - name: simple-webapp
    image: kodekloud/webapp-color
    command: ["--color","green"]controlplane $ 


9. Inspect the two files under directory 'webapp-color-3'. What command is run at container startup?


Assume the image was created from the Dockerfile in this folder

cat webapp-color-pod-2.yaml
apiVersion: v1
kind: Pod
metadata:
  name: webapp-green
  labels:
      name: webapp-green
spec:
  containers:
  - name: simple-webapp
    image: kodekloud/webapp-color
    command: ["python", "app.py"]
    args: ["--color", "pink"]


10. Create a pod with the given specifications. By default it displays a 'blue' background. Set the given command line arguments to change it to 'green'


info_outline
Hint
Pod Name: webapp-green
Image: kodekloud/webapp-color
Command line arguments: --color=green


$ kubectl run webapp-green --image=kodekloud/webapp-color --dry-run=client -o yaml > pod.yaml


vim pod.yaml

args: ["--color=green"]


$ kubectl apply -d pod.yaml

---------------------------------------------------------------------------------------------------------------


13.03.2021
==========

Solution Enviroment variable
----------------------------


1. How many PODs exist on the system?


in the current(default) namespace



$ kubectl get pods
NAME           READY   STATUS    RESTARTS   AGE
webapp-color   1/1     Running   0          2m57s

controlplane $ kubectl get pods --no-headers | wc -l
1



2. What is the environment variable name set on the container in the pod?


$ kubectl describe pod webapp-color | grep Environment -A2
    Environment:
      APP_COLOR:  pink
    Mounts:


Ans: APP_COLOR



3. What is the value set on the environment variable APP_COLOR on the container in the pod?

$ kubectl describe pod webapp-color | grep Environment -A2
    Environment:
      APP_COLOR:  pink
    

Ans: pink 


4. View the web application UI by clicking on the 'Webapp Color' Tab above your terminal.


This is located next to the Quiz Portal tab.


ok webapp-color


5. Update the environment variable on the POD to display a 'green' background


Note: Delete and recreate the POD. Only make the necessary changes. Do not modify the name of the Pod.


Pod Name: webapp-color
Label Name: webapp-color
Env: APP_COLOR=green



controlplane $ kubectl get pod webapp-color -o yaml > pod.yaml

controlplane $ vim pod.yaml 

change value pink to green


controlplane $ kubectl delete pod webapp-color 
pod "webapp-color" deleted

controlplane $ kubectl apply -f pod.yaml 
pod/webapp-color created



6. View the changes to the web application UI by clicking on the 'Webapp Color' Tab above your terminal.


If you already have it open, simply refresh the browser.


click webapp-color


7. How many ConfigMaps exist in the environment?


controlplane $ kubectl get cm
NAME        DATA   AGE
db-config   3      35s


controlplane $ kubectl get cm --no-headers | wc -l
1



8. Identify the database host from the config map 'db-config'



$ kubectl describe cm | grep -i Host -A5
DB_HOST:
----
SQL01.example.com
DB_NAME:
----
SQL01



9. Create a new ConfigMap for the 'webapp-color' POD. Use the spec given on the right.


ConfigName Name: webapp-config-map
Data: APP_COLOR=darkblue


controlplane $ kubectl create cm webapp-config-map --from-literal=APP_COLOR=darkblue

configmap/webapp-config-map created

controlplane $ kubectl get cm
NAME                DATA   AGE
db-config           3      12m
webapp-config-map   1      6s



10 .Update the environment variable on the POD use the newly created ConfigMap


Note: Delete and recreate the POD. Only make the necessary changes. Do not modify the name of the Pod.


info_outline
Hint
Pod Name: webapp-color
EnvFrom: webapp-config-map



$ kubectl explain pod --recursive | grep envFrom -A5

         envFrom        <[]Object>
            configMapRef        <Object>
            name     <string>


vim pod.yaml

change above under spec section in pod definition file


controlplane $ kubectl delete pod webapp-color 
pod "webapp-color" deleted

controlplane $ kubectl apply -f pod.yaml 
pod/webapp-color created


-----------------------------------------------------------------------------------------------------------------------

15.03.2021
==========

secrets practice test in kode kloud
-----------------------------------

1.How many Secrets exist on the system?


in the current(default) namespace


controlplane $ kubectl get secrets 
NAME                  TYPE                                  DATA   AGE
default-token-v94gx   kubernetes.io/service-account-token   3      68m


controlplane $ kubectl get secrets --no-headers | wc -l
1


2. How many secrets are defined in the 'default-token' secret?




$ kubectl describe secret default-token-v94gx | grep -i data -A3

Data
====
ca.crt:     1066 bytes
namespace:  7 bytes
controlplane $ kubectl describe secret default-token-v94gx | grep -i data -A4
Data
====
token:      eyJhbGciOiJSUzI1NiIsImtpZCI6IlBDVDdHcnBRMjdFNjdRcnJQRG5pX2J5Y012MTVzVDk4UFAzRk5BZmwxU1UifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6ImRlZmF1bHQtdG9rZW4tdjk0Z3giLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGVmYXVsdCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjI2NDJmYTcxLWUzOTAtNDY2ZC1iZTJhLTlhOWFiMDY5MjM3ZSIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDpkZWZhdWx0OmRlZmF1bHQifQ.CUGfqiSaBBgag89Zg-pIrUCcxVXNHMOOw86RDgODeJuzU0R6VP6jL_uaWlS3UvI77FODwlXQC795wKiFVdru0cJ5EwLpIqZas_cRJHi5YzD_EqtmBDvmAZYkgbIl5pnqPHqTvDqPlOT7BnfEnrWXha17TDdik8rfVA-62ZfB_CQWcAP-aqy289GIMrDn2gpNADK61SYuKGPCm6Own85xRJPVGLFCj5gsh4S5RTPqOw1tYXb8FREwp0jwdcuNX-7L9hLZAhKPoCe7BosGz3vZF60Ue1OqXExyTW_RCULoC896ZzN63e7OOGc7IoKJ1XgeAN2MFjzHawA26eJaWJciUg
ca.crt:     1066 bytes


Ans: 3



3. What is the type of the 'default-token' secret?


$ kubectl describe secret default-token-v94gx | grep -i type
Type:  kubernetes.io/service-account-token



4. Which of the following is not a secret data defined in 'default-token' secret?



Ans: Type

We are going to deploy an application with the below architecture


5. We have already deployed the required pods and services. Check out the pods and services created. Check out the web application using the 'Webapp MySQL' link above your terminal, next to the Quiz Portal Link.


$ kubectl get pods,svc

NAME             READY   STATUS    RESTARTS   AGE
pod/mysql        1/1     Running   0          69s
pod/webapp-pod   1/1     Running   0          69s

NAME                     TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
service/kubernetes       ClusterIP   10.96.0.1        <none>        443/TCP          75m
service/sql01            ClusterIP   10.107.128.229   <none>        3306/TCP         69s
service/webapp-service   NodePort    10.108.26.137    <none>        8080:30080/TCP   69s
5. 

namespace:  7 bytes 



6.The reason the application is failed is because we have not created the secrets yet. Create a new Secret named 'db-secret' with the data given(on the right).


You may follow any one of the methods discussed in lecture to create the secret.


info_outline
Hint
Secret Name: db-secret
Secret 1: DB_Host=sql01
Secret 2: DB_User=root
Secret 3: DB_Password=password123



controlplane $ kubectl create secret generic db-secret --from-literal=DB_Host=sql01 --from-literal=DB_User=root --from-literal=DB_Password=password123
secret/db-secret created


controlplane $ kubectl get secret db-secret 
NAME        TYPE     DATA   AGE
db-secret   Opaque   3      19s


7. Configure webapp-pod to load environment variables from the newly created secret.


Delete and recreate the pod if required.


info_outline
Hint
Pod name: webapp-pod
Image name: kodekloud/simple-webapp-mysql
Env From: Secret=db-secret


controlplane $ kubectl get pod webapp-pod -o yaml > pod.yaml


$ kubectl explain pods --recursive | grep -i envfrom -A8
         envFrom        <[]Object>
            configMapRef        <Object>
               name     <string>
               optional <boolean>
            prefix      <string>
            secretRef   <Object>
               name     <string>
               optional <boolean>
         image  <string>

edit pod.yaml

envFrom:

- secretRef:
        name: db-secret

$ kubectl delete pod webapp-pod 
pod "webapp-pod" deleted


$ kubectl apply -f pod.yaml 
pod/webapp-pod created


-----------------------------------------------------------------------------------------------------------------

18.03.2021
==========

init container
-------------

1. Identify the pod that has an initContainer configured.

$ kubectl describe pod blue | grep -i init
Init Containers:
  init-myservice:
  Initialized       True 
  Normal  Created    79s   kubelet, node01    Created container init-myservice
  Normal  Started    78s   kubelet, node01    Started container init-myservice


Ans: blue



2. What is the image used by the initContainer on the blue pod?

 kubectl describe pod blue | grep -i image
    Image:         busybox
    Image ID:      docker-pullable://busybox@sha256:ce2360d5189a033012fbad1635e037be86f23b65cfd676b436d0931af390a2ac
    Image:         busybox:1.28
    Image ID:      docker-pullable://busybox@sha256:141c253bc4c3fd0a201d32dc1f493bcf3fff003b6df416dea4f41046e0f37d47
  Normal  Pulling    5m7s   kubelet, node01    Pulling image "busybox"
  Normal  Pulled     5m5s   kubelet, node01    Successfully pulled image "busybox" in 2.041827278s
  Normal  Pulled     4m59s  kubelet, node01    Container image "busybox:1.28" already present on machine


Ans: busybox


3. What is the state of the initContainer on pod blue

$ kubectl describe pod blue | grep -i state
    State:          Terminated
    State:          Running

Ans: terminated


4. Why is the initContainer terminated? What is the reason?


$ kubectl describe pod blue | grep -i reason
      Reason:       Completed
  Type    Reason     Age    From               Message

Ans: Completed


5. We just created a new app named purple. How many initContainers does it have?



$ kubectl get pod purple
NAME     READY   STATUS     RESTARTS   AGE
purple   0/1     Init:0/2   0          88s


Ans: 2


6. What is the state of the POD?



 $ kubectl describe pod purple | grep -i state
    State:          Running
    State:          Waiting
    State:          Waiting

Ans: waiting



7. How long after the creation of the POD will the application come up and be available to users?

$ kubectl describe pod purple | grep -i sleep
      sleep 600
      sleep 1200
      echo The app is running! && sleep 3600

Ans: 30 mins



8. Update the pod red to use an initContainer that uses the busybox image and sleeps for 20 seconds


Delete and re-create the pod if necessary. But make sure no other configurations change.


Pod: red
initContainer Configured Correctly


$ kubectl get pod red -o yaml > red.yaml
controlplane $ kubectl delete pod red 
pod "red" deleted


vim red.yaml


apiVersion: v1
kind: Pod
metadata:
  name: red
  namespace: default
spec:
  containers:
  - command:
    - sh
    - -c
    - echo The app is running! && sleep 3600
    image: busybox:1.28
    imagePullPolicy: IfNotPresent
    name: red-container
  initContainers:
  - image: busybox
    name: red-initcontainer
    command: ["sleep", "20"]


$ kubectl apply -f red.yaml



9. A new application orange is deployed. There is something wrong with it. Identify and fix the issue.


Once fixed, wait for the application to run before checking solution.


info_outline
Hint
Issue fixed


$ kubectl get pod orange -o yaml > ornage.yaml
controlplane $ kubectl delete pod orange  
pod "orange" deleted


vim ornage.yaml


change sleeeep to sleep


 $ kubectl apply -f ornage.yaml                
pod/orange created


-------------------------------------------------------------------------------------------------------------------------


22.03.2021
==========


cluster maintenance
--------------------

practice test os upgrade on kodekloud
-------------------------------------

1. Let us explore the environment first. How many nodes do you see in the cluster?


Including the master/controlplane and workers


controlplane $ kubectl get nodes

NAME           STATUS   ROLES    AGE     VERSION
controlplane   Ready    master   2m35s   v1.19.0
node01         Ready    <none>   2m5s    v1.19.0
node02         Ready    <none>   2m6s    v1.19.0
node03         Ready    <none>   2m5s    v1.19.0


2. How many applications do you see hosted on the cluster?


Check the number of deployments



controlplane $ kubectl get deployments.apps 
NAME   READY   UP-TO-DATE   AVAILABLE   AGE
blue   3/3     3            3           48s
red    2/2     2            2           48s

controlplane $ kubectl get deployments.apps  --no-headers | wc -l
2



3. On which nodes are the applications hosted on?


$ kubectl get pods -o wide | awk '{print $7}'
NODE
node01
node02
node03
node01
node03



4. We need to take node01 out for maintenance. Empty the node of all applications and mark it unschedulable.


Node node01 Unschedulable
Pods evicted from node01


$ kubectl drain node01
node/node01 cordoned
error: unable to drain node "node01", aborting command...

There are pending nodes to be drained:
 node01
error: cannot delete DaemonSet-managed Pods (use --ignore-daemonsets to ignore): kube-system/kube-flannel-ds-amd64-zr58n, kube-system/kube-proxy-ndskz



$ kubectl drain node01 --ignore-daemonsets
node/node01 already cordoned
WARNING: ignoring DaemonSet-managed Pods: kube-system/kube-flannel-ds-amd64-zr58n, kube-system/kube-proxy-ndskz
evicting pod default/blue-746c87566d-86xcx
evicting pod default/red-75f847bf79-9nx5j
pod/blue-746c87566d-86xcx evicted
pod/red-75f847bf79-9nx5j evicted
node/node01 evicted


5. What nodes are the apps on now?


$ kubectl get pods -o wide | awk '{print $7}'
NODE
node02
node03
node02
node02
node03


6.The maintenance tasks have been completed. Configure the node to be schedulable again.



Node01 is Schedulable


 $ kubectl uncordon node01
node/node01 uncordoned


7.How many pods are scheduled on node01 now?


$ kubectl get pods -o wide | awk '{print$7}' | grep -i node01 | wc -l
0


8.Why are there no pods on node01?


new pod will schedule in nodeo1



8. Why are there no pods placed on the master node?


Check the master/controlplane node details

master and controlplane enable taints



It is now time to take down node02 for maintenance. Before you remove all workload from node02 answer the following question.


Can you drain node02 using the same command as node01? Try it.




kubectl drain node01 --ignore-daemonsets --force
node/node01 cordoned
WARNING: ignoring DaemonSet-managed Pods: kube-system/kube-flannel-ds-amd64-zr58n, kube-system/kube-proxy-ndskz
node/node01 drained


9. Why do you need to force the drain?

node 2 is part of replicaset


What is the name of the POD hosted on node02 that is not part of a replicaset?


hr-app


10. What would happen to hr-app if node02 is drained forcefully?


Try it if you wish

hr-app unscheduled


11.Drain node02 and mark it unschedulable

$ kubectl cordon node02
node/node02 cordoned


11. Node03 has our critical applications. We do not want to schedule any more apps on node03. Mark node03 as unschedulable but do not remove any apps currently running on it .



$ kubectl cordon node03
node/node03 cordoned

----------------------------------------------------------------------------------------------------------------------

26.03.2021
==========

backup-and-restore-in kubernetes
--------------------------------

1. We have a working kubernetes cluster with a set of applications running. Let us first explore the setup.


How many deployments exist in the cluster?


 $ 
controlplane $ kubectl get deployments.apps  
NAME   READY   UP-TO-DATE   AVAILABLE   AGE
blue   3/3     3            3           40s
red    2/2     2            2           40s
controlplane $ kubectl get deployments.apps --no-headers | wc -l
2



2. What is the version of ETCD running on the cluster?


Check the ETCD Pod or Process



$ kubectl -n kube-system get pods | grep -i "etcd"
etcd-controlplane                      1/1     Running   0          15m


$ kubectl -n kube-system describe pod etcd-controlplane | grep -i "image"
    Image:         k8s.gcr.io/etcd:3.4.9-1
    Image ID:      docker-pullable://k8s.gcr.io/etcd@sha256:735f090b15d5efc576da1602d8c678bf39a7605c0718ed915daec8f2297db2ff




3. At what address do you reach the ETCD cluster from your master/controlplane node?


Check the ETCD Service configuration in the ETCD POD


 $ kubectl -n kube-system describe pod etcd-controlplane | grep -i "image"
    Image:         k8s.gcr.io/etcd:3.4.9-1
    Image ID:      docker-pullable://k8s.gcr.io/etcd@sha256:735f090b15d5efc576da1602d8c678bf39a7605c0718ed915daec8f2297db2ff
controlplane $ kubectl -n kube-system describe pod etcd-controlplane | grep -i "https"
Annotations:          kubeadm.kubernetes.io/etcd.advertise-client-urls: https://172.17.0.27:2379
      --advertise-client-urls=https://172.17.0.27:2379
      --initial-advertise-peer-urls=https://172.17.0.27:2380
      --initial-cluster=controlplane=https://172.17.0.27:2380
      --listen-client-urls=https://127.0.0.1:2379,https://172.17.0.27:2379
      --listen-peer-urls=https://172.17.0.27:2380


Ans: https://127.0.0.1:2379




4. Where is the ETCD server certificate file located?


Note this path down as you will need to use it later


 $ kubectl -n kube-system describe pod etcd-controlplane | grep -i "crt"  
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt


5.Where is the ETCD CA Certificate file located?


Note this path down as you will need to use it later


 $ kubectl -n kube-system describe pod etcd-controlplane | grep -i "crt"  
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt



6. The master nodes in our cluster are planned for a regular maintenance reboot tonight. While we do not anticipate anything to go wrong, we are required to take the necessary backups. Take a snapshot of the ETCD database using the built-in snapshot functionality.


Store the backup file at location /opt/snapshot-pre-boot.db



Backup ETCD to /opt/snapshot-pre-boot.db




$ ETCDCTL_API=3 etcdctl --cacert="/etc/kubernetes/pki/etcd/ca.crt" --cert="/etc/kubernetes/pki/etcd/server.crt" --key="/etc/kubernetes/pki/etcd/server.key" snapshot save /opt/snapshot-pre-boot.db


{"level":"info","ts":1616739294.7114887,"caller":"snapshot/v3_snapshot.go:119","msg":"created temporary db file","path":"/opt/snapshot-pre-boot.db.part"}
{"level":"info","ts":"2021-03-26T06:14:54.724Z","caller":"clientv3/maintenance.go:200","msg":"opened snapshot stream; downloading"}
{"level":"info","ts":1616739294.7277067,"caller":"snapshot/v3_snapshot.go:127","msg":"fetching snapshot","endpoint":"127.0.0.1:2379"}
{"level":"info","ts":"2021-03-26T06:14:54.767Z","caller":"clientv3/maintenance.go:208","msg":"completed snapshot read; closing"}
{"level":"info","ts":1616739294.7952766,"caller":"snapshot/v3_snapshot.go:142","msg":"fetched snapshot","endpoint":"127.0.0.1:2379","size":"3.2 MB","took":0.083574474}
{"level":"info","ts":1616739294.795515,"caller":"snapshot/v3_snapshot.go:152","msg":"saved","path":"/opt/snapshot-pre-boot.db"}
Snapshot saved at /opt/snapshot-pre-boot.db




7. Wake up! We have a conference call! After the reboot the master nodes came back online, but none of our applications are accessible. Check the status of the applications on the cluster. What's wrong?


Deployment services pods are not present


8. Luckily we took a backup. Restore the original state of the cluster using the backup file.


Deployments: 2
Services: 3




$ EDCDCTL_API=3 etcdctl snapshot restore /opt/snapshot-pre-boot.db --data-dir=/var/lib/etcd-from-backup

{"level":"info","ts":1616739627.2521791,"caller":"snapshot/v3_snapshot.go:296","msg":"restoring snapshot","path":"/opt/snapshot-pre-boot.db","wal-dir":"/var/lib/etcd-from-backup/member/wal","data-dir":"/var/lib/etcd-from-backup","snap-dir":"/var/lib/etcd-from-backup/member/snap"}
{"level":"info","ts":1616739627.2963548,"caller":"mvcc/kvstore.go:380","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":4093}
{"level":"info","ts":1616739627.3152945,"caller":"membership/cluster.go:392","msg":"added member","cluster-id":"cdf818194e3a8c32","local-member-id":"0","added-peer-id":"8e9e05c52164694d","added-peer-peer-urls":["http://localhost:2380"]}
{"level":"info","ts":1616739627.3587039,"caller":"snapshot/v3_snapshot.go:309","msg":"restored snapshot","path":"/opt/snapshot-pre-boot.db","wal-dir":"/var/lib/etcd-from-backup/member/wal","data-dir":"/var/lib/etcd-from-backup","snap-dir":"/var/lib/etcd-from-backup/member/snap"}



$ kubectl get pods
No resources found in default namespace.

change host path in etcd file



 cd /etc/kubernets/manifests

vim etcd.yaml

hostpath: /var/lib/etcd-from-backup


save


$ docker ps -a | grep etcd


$ docker logs conatinerid


$ sudo docker ps


------------------------------------------------------------------------------------------------------------------------

05.04.2021
==========

Tls certificate in kubernetes
-----------------------------

$ minikube start

$ minikube status

$ minikube addons list

# enable ingress addons

$ minikube addons enable ingress

to check

$ kubectl get pods -n kube-system


create deployment


$ kubectl run nginx --image=nginx

$ kubectl expose deployment nginx --port 80

vim ingress.yaml

apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: nginx
  
spec:
  
  rules:
  - host: saiva.com
    http:
      paths:
      - backend:
          serviceName: nginx
          servicePort: 80 



$ kubectl apply -f ingress.yaml


$ echo "$(minikube ip) saiva.com" | sudo tee -a /etc/hosts

$ cat /etc/hosts | tail -n1


Access the url


$ curl saiva.com


create tls certificate
---------------------

openssl req -x509 -newkey rsa:4096 -sha256 -nodes -keyout tls.key -out tls.crt -subj "/CN=saiva.com" -days 365 


create secret
------------

$ kubectl create secret tls saiva-com-tls --cert=tls.crt --key=tls.key


$ kubectl get secret -o yaml


vim ingress.yaml

apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: nginx
  
spec:
  tls:
    - secretName: saiva-com-tls1
      hosts:
      - saiva.com 
  rules:
  - host: saiva.com
    http:
      paths:
      - backend:
          serviceName: nginx
          servicePort: 80


$ kubectl apply -f ingress.yaml


curl -k https://saiva.com

curl --cacert tls.crt https://saiva.com

----------------------------------------------------------------------------------------------------

09.04.2021
==========

RBAC IN KUBERNETES
------------------

vim developer-role.yaml

apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  # "namespace" omitted since ClusterRoles are not namespaced
  name: developer
rules:
- apiGroups: [""]
  #
  # at the HTTP level, the name of the resource for accessing Secret
  # objects is "secrets"
  resources: ["pods"]
  verbs: ["get", "watch", "list"]
- apiGroups: [""]

  resources: ["configMap"]
  verbs: ["create"]
    

$ kubectl apply -f developer-role.yaml



$ kubecl get roles

NAME        CREATED AT
developer   2021-04-09T06:42:35Z



vim devuser-developer-role-binding.yaml


apiVersion: rbac.authorization.k8s.io/v1
# This role binding allows "jane" to read pods in the "default" namespace.
# You need to already have a Role named "pod-reader" in that namespace.
kind: RoleBinding
metadata:
  name: devuser-deveolper-binding
  namespace: default
subjects:
# You can specify more than one "subject"
- kind: User
  name: devuser # "name" is case sensitive
  apiGroup: rbac.authorization.k8s.io
roleRef:
  # "roleRef" specifies the binding to a Role / ClusterRole
  kind: Role #this must be Role or ClusterRole
  name: developer # this must match the name of the Role or ClusterRole you wish to bind to
  apiGroup: rbac.authorization.k8s.io


$ kubectl apply -f devuser-develper-role-binding.yaml

 kubectl get rolebindings
NAME                        ROLE             AGE
devuser-deveolper-binding   Role/developer   3h37m

$ kubectl describe role developer


$ kubectl describe rolebinding devuser-developer-role-binding


check your roles
----------------

$ kubectl auth can-i create deployments

yes

$ kubectl auth can-i delete nodes

no


$ kubectl auth can-i create deployments --as devuser

no

----------------------------------------------------------------------------------------------------------------

12.04.2021
==========

network policy in kubernets
---------------------------

$ kubect get netpol 

$ kubectl describe netpol payroll


$ kubectl get pods -l name=payroll


vim inernal-policy.yaml


apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: internal-policy
  namespace: default
spec:
  podSelector:
    matchLabels:
      name: internal
  policyTypes:
  - Egress
  
  egress:
  - to:
           
    - podSelector:
        matchLabels:
          name: mysql
    ports:
    - protocol: TCP
      port: 3306

  - to:

         
    - podSelector:
        matchLabels:
          name: payroll
    ports:
    - protocol: TCP
      port: 8080


kubctl apply -f internal-policy.yaml


kubectl describe netpol payroll-policy


----------------------------------------------------------------------------------------------------------------------------

16.04.2021
----------


Configure a Pod to Use a PersistentVolume for Storage
-----------------------------------------------------

Create an index.html file on your Node

minikube ssh

sudo mkdir /mnt/data

sudo sh -c "echo 'Hello from Kubernetes storage' > /mnt/data/index.html"

cat /mnt/data/index.html

Hello from Kubernetes storage



Create a PersistentVolume
------------------------

vim pv-volume.yaml

apiVersion: v1
kind: PersistentVolume
metadata:
  name: task-pv-volume
  labels:
    type: local
spec:
  storageClassName: manual
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/mnt/data"


$ kubectl  apply -f pv-volume.yaml

$ kubectl get pv task-pv-volume


Create a PersistentVolumeClaim
------------------------------

vim pv-claim.yaml

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: task-pv-claim
spec:
  storageClassName: manual
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 3Gi


$ kubectl apply -f pv-claim.yaml


$ kubectl get pv task-pv-claim

Now the output shows a STATUS of Bound.


$ kubectl get pvc task-pv-claim



Create a Pod
The next step is to create a Pod that uses your PersistentVolumeClaim as a volume.

vom pv-pod.yaml

apiVersion: v1
kind: Pod
metadata:
  name: task-pv-pod
spec:
  volumes:
    - name: task-pv-storage
      persistentVolumeClaim:
        claimName: task-pv-claim
  containers:
    - name: task-pv-container
      image: nginx
      ports:
        - containerPort: 80
          name: "http-server"
      volumeMounts:
        - mountPath: "/usr/share/nginx/html"
          name: task-pv-storage


Create the Pod:

$ kubectl apply -f pv-pod.yaml

Verify that the container in the Pod is running;

$ kubectl get pod task-pv-pod


Get a shell to the container running in your Pod:

$ kubectl exec -it task-pv-pod -- /bin/bash

# Be sure to run these 3 commands inside the root shell that comes from
# running "kubectl exec" in the previous step
apt update
apt install curl
curl http://localhost/


Clean up

Delete the Pod, the PersistentVolumeClaim and the PersistentVolume:

kubectl delete pod task-pv-pod
kubectl delete pvc task-pv-claim
kubectl delete pv task-pv-volume


If you don't already have a shell open to the Node in your cluster, open a new shell the same way that you did earlier.

In the shell on your Node, remove the file and directory that you created:

# This assumes that your Node uses "sudo" to run commands
# as the superuser
sudo rm /mnt/data/index.html
sudo rmdir /mnt/data


-----------------------------------------------------------------------------------------------------------------

23.04.2021
==========

Deploy network solution in kubernetes in kodekloud
---------------------------------------------------


1. In this practice test we will install weave-net POD networking solution to the cluster. Let us first inspect the setup.


We have deployed an application called app in the default namespace. What is the state of the pod?


$ kubectl get pods
NAME   READY   STATUS              RESTARTS   AGE
app    0/1     ContainerCreating   0          53s

Notrunning


2. Inspect why the POD is not running


$ kubectl describe pod app | grep network
  Warning  FailedCreatePodSandBox  2m35s                kubelet, node01    Failed to create pod sandbox: rpc error: code = Unknown desc = [failed to set up sandbox container "7965c42d8b313820557fea3774d1ecd34848b84df8f5dfd51ec1cfefee2b225b" network for pod "app": networkPlugin cni failed to set up pod "app_default" network: unable to allocate IP address: Post "http://127.0.0.1:6784/ip/7965c42d8b313820557fea3774d1ecd34848b84df8f5dfd51ec1cfefee2b225b": dial tcp 127.0.0.1:6784: connect: connection refused, failed to clean up sandbox container "7965c42d8b313820557fea3774d1ecd34848b84df8f5dfd51ec1cfefee2b225b" network for pod "app": networkPlugin cni failed to teardown pod "app_default" network: Delete "http://127.0.0.1:6784/ip/7965c42d8b313820557fea3774d1ecd34848b84df8f5dfd51ec1cfefee2b225b": dial tcp 127.0.0.1:6784: connect: connection refused]


network not configured



Deploy weave-net networking solution to the cluster


Check the documentation here


$ kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')"


serviceaccount/weave-net created
clusterrole.rbac.authorization.k8s.io/weave-net created
clusterrolebinding.rbac.authorization.k8s.io/weave-net created
role.rbac.authorization.k8s.io/weave-net created
rolebinding.rbac.authorization.k8s.io/weave-net created
daemonset.apps/weave-net created


$ kubectl -n kube-system get pods | grep weave
weave-net-q4djz                        2/2     Running   0          63s
weave-net-xhfcz                        2/2     Running   0          63s


$ kubectl get pods
NAME   READY   STATUS    RESTARTS   AGE
app    1/1     Running   0          8m56s


-------------------------------------------------------------------------------------------------------------

30.04.2021
==========

ingress
-------

1. We have deployed two applications. Explore the setup.
Note: They are in different namespaces.


kubectl get deployments.apps --all-namespaces

NAMESPACE     NAME              READY   UP-TO-DATE   AVAILABLE   AGE
app-space     default-backend   1/1     1            1           36s
app-space     webapp-video      1/1     1            1           36s
app-space     webapp-wear       1/1     1            1           36s
kube-system   coredns           2/2     2            2           70m



2.Let us now deploy an Ingress Controller. First, create a namespace called 'ingress-space'
We will isolate all ingress related objects into its own namespace.

Name: ingress-space


$ kubectl create ns ingress-space
namespace/ingress-space created

$ kubectl get ns ingress-space 
NAME            STATUS   AGE
ingress-space   Active   38s



3. The NGINX Ingress Controller requires a ConfigMap object. Create a ConfigMap object in the ingress-space.
Use the given spec on the right. No data needs to be configured in the ConfigMap.

Name: nginx-configuration


$ kubectl create cm nginx-configuration  -n ingress-space 
configmap/nginx-configuration created



4. The NGINX Ingress Controller requires a ServiceAccount. Create a ServiceAccount in the ingress-space.
Use the given spec on the right.

Name: ingress-serviceaccount


$ kubectl create sa ingress-serviceaccount -n ingress-space 
serviceaccount/ingress-serviceaccount created



5. We have created the Roles and RoleBindings for the ServiceAccount. Check it out!


$ kubectl -n ingress-space get roles.rbac.authorization.k8s.io 
NAME           CREATED AT
ingress-role   2021-04-30T08:21:34Z



6. Let us now deploy the Ingress Controller. Create a deployment using the file given.
The Deployment configuration is given at /root/ingress-controller.yaml. There are several issues with it. Try to fix them.

Deployed in the correct namespace.
Replicas: 1
Use the right image


vim 

 /root/ingress-controller.yaml


change apiVersion: apps/v1

namespace: ingress-space

$ kubectl apply -f ingress-controller.yaml 
deployment.apps/ingress-controller created


7. Let us now create a service to make Ingress available to external users.
Create a service following the given specs.

Name: ingress
Type: NodePort
Port: 80
TargetPort: 80
NodePort: 30080
Use the right selecto


$ kubectl -n ingress-space expose deployment ingress-controller --name ingress --port 80 --target-port 80 --type Nodeport --dry-run=client -o yaml > ingress-svc.yaml


vim ingress-svc.yaml

apiVersion: v1
kind: Service
metadata:
  creationTimestamp: null
  name: ingress
  namespace: ingress-space
spec:
  ports:
  - port: 80
    protocol: TCP
    targetPort: 80
    nodePort: 30080
  selector:
    name: nginx-ingress
  type: NodePort 
status:
  loadBalancer: {}




$ kubectl apply -f ingress-svc.yaml 
service/ingress created



8. Create the ingress resource to make the applications available at /wear and /watch on the Ingress service.
Create the ingress in the app-space

Ingress Created
Path: /wear
Path: /watch
Configure correct backend service for /wear
Configure correct backend service for /watch
Configure correct backend port for /wear service
Configure correct backend port for /watch service


vim ingress-resource.yaml

apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  namespace: app-space
  name: test-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - http:
      paths:
      - path: /wear
        pathType: Prefix
        backend:
          service:
            name: wear-service 
            port:
              number: 8080
      - path: /watch
        pathType: Prefix
        backend:
          service:
            name: video-service
            port: 
              number: 8080 


$ kubectl apply -f ingress-resource.yaml 
ingress.networking.k8s.io/test-ingress created


Access via browser

https://2886795277-30080-kitek29z.environments.katacoda.com/wear

https://2886795277-30080-kitek29z.environments.katacoda.com/watch


------------------------------------------------------------------------------------------------------------

05.05.2021
==========

kubernetes clutser using kubeadm in kodekloud
--------------------------------------------

1. Install the kubeadm package on master and node01

   
    kubeadm installed on Master?
    Kubeadm installed on worker node01 

on master node
-------------

vim kubeadm-install.sh

#!/bin/bash
##Update the apt package index and install packages needed to use the Kubernetes apt reposito
sudo apt-get update
sudo apt-get install -y apt-transport-https ca-certificates curl

##Download the Google Cloud public signing key:

sudo curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.go

##Add the Kubernetes apt repository:

echo "deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernet

##Update apt package index, install kubelet, kubeadm and kubectl, and pin their version:

sudo apt-get update
sudo apt-get install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl


sh kubeadm-install.sh


$ kubeadm version -o short
v1.21.0

$ kubelet --version
Kubernetes v1.21.0


on worker node
---------------

vim kubeadm-install.sh

#!/bin/bash
##Update the apt package index and install packages needed to use the Kubernetes apt reposito
sudo apt-get update
sudo apt-get install -y apt-transport-https ca-certificates curl

##Download the Google Cloud public signing key:

sudo curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.go

##Add the Kubernetes apt repository:

echo "deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernet

##Update apt package index, install kubelet, kubeadm and kubectl, and pin their version:

sudo apt-get update
sudo apt-get install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl


sh kubeadm-install.sh


$ kubeadm version -o short
v1.21.0

$ kubelet --version
Kubernetes v1.21.0


2. What is the version of kubelet installed?


$ kubelet --version
Kubernetes v1.21.0



3. 

How many nodes are part of kubernetes cluster currently?

Are you able to run kubectl get nodes?


$ kubectl get nodes
error: the server doesn't have a resource type "nodes"

Ans: 0


4. Initialize Control Plane Node (Master Node)

Once done, set up the default kubeconfig file and wait for node to be part of the cluster.

    info_outline
    Hint

    Master node initialized 

$ kubeadm init 

mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

ssh node01

kubeadm join 172.17.0.24:6443 --token ktcszn.sumtqgtudcjhg8q9 \
        --discovery-token-ca-cert-hash sha256:293101bf857277120647825f34660f81b9ec94fb8a737a3c65bcf5e3b50649d9 


master node


kubectl get nodes
NAME           STATUS   ROLES                  AGE   VERSION
controlplane   Ready    control-plane,master   2m    v1.21.0
node01         Ready    <none>                 44s   v1.21.0

----------------------------------------------------------------------------------------------------------------------






























































































