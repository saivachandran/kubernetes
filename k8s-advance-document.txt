kubernetes certified Administrator
=================================

28.12.2020
==========

course introduction
------------------

pre-requisties
--------------

1. docker

2. basic kubernetes

  . pod
  . Deployments
  . services

3. yaml

4. setting basic of vm



course objective
----------------

1.cluster architecture

2. Api primitives

3. services and other network primitives 	 	

4. scheduling

  . labels & selector
  . Daemon sets
  . Resuorce limits
  . multiple schedulers
  . Manual scheduling
  . scheduler Events
  . configure Kubernetes scheduler

4. logging monitoring

   . Monitor cluster components
   . Monitor cluster component logs
   . Monitor Applications
   . Applications Logs

5. Application Life cycle management

   . Rolling updates and rollbacks in Deploy
   . configure Applications
   . scale Applications
   . self-healing applications
6. cluster maintenance

   . cluster upgrade process
   . operting system upgrades
   . backup nad restore methodlogy

7. security
   
   . Authendication and Autherization
   . kubernetes securities
   . Network Policies
   . Tls certicates and cluster components
   . images securly
   . secure contexts
   . secure persistence value store

8. storage

  . Persistent Volume
  . Access Modes of volumes
  . Persistent Volume claim
  . Kubernetes storage object
  . configure application with persistence volumes

9. Networking
   . pre-requisties Network switching routing tools
   .  pre-requisties Dns and core Dns
   . Network configuring on cluster nodes
   . Pod Networking concepts
   . service Networking
   . Network Load balancer
   . ingres
   . Network Namespace
   . bridge networks
   . core dns
   . cni
   . networking docker

10. installation configuration and validation

   . Design a kubernetes cluster
   . secure kubernetes communication
   . provision infrastructure
   . install kubernetes master and nodes 
   . HA kubernetes Cluster
   . choose Network solution
   . Node end to end tests
   . Run and Analyze end to end test

11. trouble shooting

   . Application failure
   . control plane failure
   . Worker Node failure
   . Networking

  

Kubernetes certifications
========================

As per recent survey kubernetes fastest grow in recent job search

kubernetes certified administrator developed by cloud native computing foundation 

collabration by linux foundation


To read more about certication visit below website

https://www.cncf.io/certification/cka/

exam cost 300 us dollar one retake option with in 12 months


Requirement need to met
-----------------------
to check enviroment to attend exam  system requirement and network connectivity

Handbook available in certification website

unlike other certication kubernetes not a multiple choices exam

it's performance based exam to test your handson skills on kubernetes

you need to know hoe technogy work and how you going to work

3 hours time for exam	

we refer kubernetes official document during exam 



30.12.2020
==========

kubernetes architecture
=======================

** we start with basic overview of kubernetes cluster

** first we look at architecture at high level and drill down each component one by one

Cluster Archticture
-------------------

  . ETCD for beginners
  . ETCD in kubernetes
  . Kube API server
  . controller Mananger
  . kube scheduler
  . kubelet
  . kube proxy


** we are going to use ships to anaylsis archicture  of kubernetes

** purpose of kubernetes is host your application in the form of containers

** Easily deploy many instances of your applications its require enable communications between different services

** so many things involve work together it's possible

** we have two kind of ships cargo ships actual work carring container

** controller ships resposible managing and monitoring gargo ships

** kubernetes cluster consist of set of nodes physical , virtual, on permises, cloud

** worker nodes is responsible for load container in the ships

** somebody need to load container not just load plan how to load, identify the rigt ships, store information about ships, track the location of the continer on the ships this is done by controll ships

** controll ships will lay to master node in the kubernetes cluster

** master node managing kubernetes cluster, storing inforation regarding different nodes, montoring nodes, master node done all of these using control plane component 

** many container loaded and unloaded in daily basis so need to maintain information what container on which ships, which time it's loaded etc

** all of this stored highly available key value stored called ETCD cluster 

** ETCD is a database store inforation in key value format

** kube-scheduler in kubernetes cluster identify the right node to place a container, resuorce requirments, worker node capacity or any other policies


controller available in kubernetes 
---------------------------------

. Node-controller take care of nodes on borading new node on the cluster, node become unavailable, destroy node

. replication controller take care of desire of container on cluster 

. how to communicate each other 

. kube-api server primary management component of kubernetes it's responsible all operation in the kubernetes cluster

. kubernetes api server used by external user manage kuebrnetes, various controller kubernetes make nessesary changes in cluster

. master node, networking solution, dns  all are deployed in container format 

. container run time engine popular one docker 

. kubernetes supports other runtime engine like conatinerd or rockit


. Every gargo ships gas captain captain responsible for all activities in the ships

. captain of kubernrtes in kubelet

. kubelet is agent run each node on cluster listen instruction from kube-apiserver, deploy or destroy container on nodes

. kube apiserver periodically fetches status report from kubelet montor status of nodes,containers

. application running on worker node to communicate each other

. web server ruuning on one container one node, db server running another conatiner another node

. how web server reach db server on another node

. communication between service with in the cluster enable by another component called kube-proxy


31.12.2020
==========

ETCD introduction
=================

objectives
---------

1. what etcd

2. what is key-value store

3. How to get started quickly

4. how to operate etcd

later
-----
1. what distributed system

2. How etcd operates

3. raft protocol

4. Best practice on number of nodes



what is ETCD
============

it's disributed reliaible key value store that's simple secure and fast


what is key-value store
======================

** databases in tabular format you hear about mysql and relational databases

** store data form of rows and columns row represent person column represent type of information

**  key value store information key and value format


key   value

name   saiva

age    30

** we can't have duplicate keys it's used to retrive small jungs of data such as configuration data


install ETCD
============

1. Download binaries from github repo

2. extract

3. Run ETCD service

ETCd port : 2379


default client comes with ETCd controll client  ./etcdctl set key value


etcd controll client it's command client for etcd 


retrive information

./etcdctl get key

value

to view more option ./etcdctl command without any option

./etcdctl

-------------------------------------------------------------------------------------------

ETCD role in kubernetes
=======================

ETCD data store store information regarding cluster such as 

1. nodes

2. pods

3. config

4. secrets

5. Accounts

6. roles

7. bindings

6. others


when you kubssl commands details get from ETCD cluster 

you adding additional pod, replicasets,deployments all are updated in ETCD cluster

depending on how you setup cluster ETCD deploy differently

one deploy from scratch aother from kubeaadm tool


later we stup cluster using scratch


ETCD setup manual scratch
========================

wget https://github.com/etcd-io/etcd/releases/download/v3.4.14/etcd-v3.4.14-linux-amd64.tar.gz

we have whole section on tls certicate other section for configuration

etcd.service

notice advertise client url

listen on server ip and port 2379


setup using kubeadm
===================

kubeadm deploy etcd as pod kube-system

$ kubectl get pods kube-system

$ kubectl exec etcd-master -n kube-system etcdctl get / --prefix -keys-only


kubernetes store data in specific directory under root dirctory have registry 	

Registry

  minions
  pods
  replicasets
  deployments
  roles
  secrets


ETCD HA Enviroment
=================

High availability enviroment you have multiple master node 

multible etcd instances cross the maximum nodes

ETCD instances know about each other set correct parameter etcd service configuration

initial cluster options set different service options

-----------------------------------------------------------------------------------------------------
02.01.2021
==========

kube-api-server in kubernetes
============================

. kube-api-server is primary management component in kubernetes

. when you run kubectl command kube controll utility reach kube-api-server

. kube-api-server first authendicate request and validate

. retrive data from etcd cluster response back with requested information

. instead you can send api post request directly api create pod object without assging to node

.  update information in the etcd server and update information to user the pod is created 

. the scheduler continously monitor the api server there is new pod with no node assign 

. schedluer identify the right node to place the new node on and communicate tahe back to api-server

. api server update the information in the etcd cluster

. then api-server update to kubelet in the appropriate worker node

. kubelet create pod in the node and instruct container run time engine to deploy application

. once done kubelet update to api server

. api-server then update to etcd cluster

. every time the change is requested kube-api-server centre of all different tasks. tasks need to perform change in the cluster

. kube-api-server responsible authendicating and validating, retriving, updating datastore 	

. kube-api-server is only component directly intract with etcd datastore

. other compontent schdeuler, kubelet, kubecontrol manager  uses the api server update in the cluster to perform updates in the cluster their 

respective areas



. view api-server options- kubeadm 

cat /etc/kubernetes/manifests/kube-apiserver.yaml


. non kueadm setup

cat /etc/systemd/system/kube-apiserver.service

. also see the running process

ps -aux | grep kube-apiserver


02.01.2021
==========

kube-controller-Manager
======================

. kube controller manager controll various controller in kubernetes

. controller is office or department in ships that have own responsibilities

. office for the ships monitoring and take necessary actions manage the container on the ships


kubernetes controller
====================

. controller is process contiously monitor the state of the various component 

. work towards the desire functions of all components


Node controller
--------------

. Node controller responsible monitor the state of the node take nessacery action keep application running

. node controller check status of the node via apiserver

. Node controller check the status node every 5 seconds that way node controller can monitor the health of the nodes

$ kubectl get nodes

. if stop receving heart beat from node marked as node is unreachable

. but wait 40 secs before mark unreachable

. afetr mark unreachable node have 5 mins to come back

. pod not up with in 5 mins node controller remove that pod assign new pod to healthy node


Replication controller
=====================

. it's responsible for monitor the status of replicaset ensure desire number of pod available all time

. if pod dies it create another one

. those two example of controller many more such a controller available in kubernetes


3. Deployment controller

4. Namespce controller

5. Endpoint controller

6. Service-Account-controller

7. pv-protection-controller

8. pv-Binder-controller

9. cron job

10. job controller

11. replicaset

12. stateful-set


all controller packages in to single process known as kubernetes controller manager

. install kubernetes controller manager different controller install as well


. download kube-controller manager from kubernetes release page and install run it as a service


how to view kube-controller managerserver option - kubeadm
----------------------------------------------------------

$ kubectl get pods -n kube-system

$ cat /etc/kubernetes/manifests/kube-controller-manager.yaml


non kubeadmin setup
------------------

$ cat /etc/systemd/system/kube-controller-manager.service


also see the running process

ps -aux | grep 	kube-controller-manager

-------------------------------------------------------------------------------------------------
04.01.2020
==========

kube scheduler
=============

. kubernetes scheduler responsible for scheduling pod on right node

. scheduler only decide which pod goes which node

. it doesn't actually place the pod on the node, place the pod on the node done is by kubelet

. kubelet are the captain on the ships who creates the pod, scheduler only decide which pod goes where


*** how schedluer does that in bit more details

. why do we need scheduler

. we have many container and many ships, scheduler select right container on right ships

. we have different sizes of container and we need right capacity ships to accomidate that container


. in kubernetes scheduler decide which node pod placed on depends on certain cretieria

. we may have pods different resource requirement, nodes in the cluster dedicated to certain applications, how to scheduler assign this pod

. scheduler looks each pod and try to find best node for it

. we look one pod it has set of cpu and memeory requirement

. we have two phases to identify the best node of the pod

   1. filer the pod

     . filer out the node the do not fit the pod profile
     . node don't have sufficient cpu and memory rquested by pod are removed, now have two nodes which pods can be placed
     . how scheduler pick one from two 

   2. Rank nodes

      . scheduler rank the nodes to identify the best feet for the pod assign the score from 0 to 10 with priority function
      . schedluer calculate amount of free resource after place the pod, high cpu free is got selected 


. resource requirements and limits

. Taints and tolerations

. Node selector/Affinity


How you install kube scheduler as a service

. download binaries from kubernetes schedluer and extract install 

How you view kube scheduler option

  . cat /etc/kubernetes/manifests/kube-scheduler.yaml

view running process

$ ps -aux | grep kube-scheduler

-----------------------------------------------------------------------------------------------------------

Kubelet
======

** kubelet is like a captain on the ships, kubelet lead all activity on the ships

** they once responsible for all paperwork being part of the cluster

** they load and unload conatiner on the ships

. kubelet in the kubernet worker node register node with kubernetes cluster

. when it receive load a container or pod on the node, it request container runtime engine docker pull image and create instance

. kubelet contionusly monitor the state of the pod and node

. report the kube-apiserver timely based


how do you install kubelet

. if you use kubeadm tool to deploy a cluster  kubeadm not automaticaly deploy kubelet

. you msust manually install on your worker node

download installer extract it install as a service

you can running process of kubelet

ps -aux | grep kubelet

-----------------------------------------------------------------------------------------

Kube-proxy
================
. kube-proxy is a network proxy that runs on each node in your cluster, implementing part of the Kubernetes Service concept. kube-proxy maintains network rules on nodes. These network rules allow network communication to your Pods from network sessions inside or outside of your cluster

. within kubernetes cluster every pod can reach another pod but this complexed by deploying pod networking solution

. pod network is a internal virtual network to across all node in the cluster to all pods connected throug this they able to communicate

each other

. many solution available deploying such a network

. this case have a web application deployed on first node, database application deployed on second

. webapp can reach database using ip of the pod, there is no guaranty ip of the pod remain same

. beginer course we use services access database using name of the service db 

. service also get an ip address, whenever pod tries to reach service using it's ip forward traffic to backend pod this case the database

. what is service how does get an ip, service can't join pod network, because service not an actual thing

. it's not a container like pod it doesn't have any interfaces or actively listening process, it's virtual component only listen kubernetes memeory

. service is accessible across cluster from any node, so how that acheived here kube-proxy comes in

kube-proxy 
---------

. kube-proxy is a process running on each node in the kubernetes cluster

. it's job look for new services, every time new services created it's create appropriate rule on each node forward traffic to services to the

backend pod
 
. one way does this using ip tables, it create ip tables rules each node on the cluster to forward traffic heading to the ip thats how kubeproxy

configure service

how to install kube-proxy

download kubeproxy binary from kubernetes release page extract it run it as a service

. kubeadm tool deploy kube-proxy as a pod infact it's deployed as daemon set

-----------------------------------------------------------------------------------------------------------

06.01.2020
==========

Pod Recap
=========

. Assume application already developed and build into docker images available in dockhub public repository

. kubernetes can pull it down, also assume kubernetes cluster alreadt set up and it's working 

. this could be single node setup or multinode setup doesn't matter services need to be running state

. already discuss kubernetes ultimate aim deploy application in a container format on a set of machines there a worker node in a cluster

. however kubernetes doesn't deploy container directly on the worker node

. container are encapsulated in to kubernetes object called pods

. pod is a single instance of the application, pod is smallest object that you can create in kubernetes


 Here we see single node kubernetes cluster with a single instance of your appllication in single docker container encapsulated with  pod

  . if the number of user accessing you application increases need to scale your application
 
  . need to add additional instances for your web applications to share the load

  . where we spinup new instances bring new container with in pod no, new bring new pod with new instances of application on same node

  . if user further increase current node don't have sufficient capacity, kubernetes deploy new pod on another node, node should be added in cluster to expand physical capacity 

  . pod have one to one relatinships container running in your applications

  . scale up you create new pod to scale down delete existing pod


 Multicontainer pod
  
 . pod have multiple containers

 . sometime we need helper container processing user end data , handling file uploaded by user etc, that case you two containers part of the same pod

  . two containers easily share same network namspace and storage volumes


how to deploy pod

   $ kubectl run nginx --image=nginx

. kubect command does deploying docker container by creating pod, create pod automatically deploy nginx docker image automatically 

. where we get application image from docker hub repository , dockerhub is a public repository various images and applications stored


view list of pods

$ kubectl get pods

--------------------------------------------------------------------------------------

07.08.2020
==========

Pod with yaml
=============

. Here we see creating pod using yaml facing configuration 

. kubernetes uses yaml file as input create object called pod,replicaset, Deployment,services,etc

vim pod-defintion.yml

apiVersion: v1

kind: Pod

metadata: 

  name: myapp-pod
  labels:
    app: myapp
    type: front-end

spec:
  containers:
    name: nginx-container
    image: nginx


$ kubectl create -f pod-defintion.yml

$ kubectl get pods

$ kubect describe pod myapp-pod

-------------------------------------------------------------------------------------------

20.01.2021
===========

Recap Replicasets
-----------------

 we discuss about kubernetes controller
 --------------------------------------

. Controller brain behind the kubernetes, process monitor kubernetes object response accordingly, this topic we discuss about one controller 

thats relication controller

. so what is replica why do we need replication controller

. go back first scenerio we running single pod for application some reasons our applications pods fail, users can't access applications

. to prevent user lose access we would like to have more than one instances or pod running at the same time that way if one fail still our

 application running at another one 

. replication controller helps us run multiple instances of single pod cluster doses providing high availability

. even single pod can help if existing pod fail, it bring new pod, replication controller ensure specified number of pod running all time

. replication controller span across multible nodes, node don't sufficient resource, scale our application when demand increases


. important to know replication controller and replicaset both have purpose

. replication controller is a owner technology being replaced by replicasets

. replicaset new recommended way to set replication there are minor differnece each of these works



replication controller yaml file format
---------------------------------------

apiversion = apiversion specific what we creating, replication controller in kubernetes supports apiversion v1


vim rc-definition.yaml

apiVersion: v1
kind: ReplicationController
metadata:
  name: myapp-rc
  labels:
    name: myapp
    type: front-end

spec:
  template:
    
    metadata: 
      name: web
      labels:
      
        name: web
        tier: front-end
    spec:
      containers:   
        - name: nginx
          image: nginx
  replicas: 3      
      

$ kubectl create -f rc-definition.yaml

$ kubectl get rc

$ kubectl get pods

---------------------------------------------------------------------------------------

vim replicaset-definition.yaml

apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: myapp-replicaset
  labels:
    name: myapp
    type: front-end

spec:
  template:
    
    metadata: 
      name: myapp
      labels:
        name: myapp
        tier: front-end
    spec:
      containers:   
        - name: nginx-container
          image: nginx
  replicas: 3 
  selector:
    mathchlabels:
      type: front-end


$ kubectl create -f replicaset-definition.yaml

$ kubectl get replicaset

$ kubectl get pods


scale replicasets
-----------------

1. update number of replicas in definition file change 3 to 6

$ kubectl replace -f replicaset-definition.yaml

$ kubectl scale --replicas=6 -f replicaset-definition.yaml

$ kubectl scale --replicas=6  replicset myapp-replicaset


commands
--------

$ kubectl create -f replicaset-definition.yaml

$ kubectl get replicasets

$ kubectl delete replicaset replicaset-definition.yaml

$ kubectl replace -f replicaset-definition.yaml

$ kubectl scale --replicas=6 replicaset myapp-replicaset


-------------------------------------------------------------------------------------------------------

25.01.2021
==========

Recap-Deployment
----------------

. Discus about how you deploy your application in production enviroment

. have webserver need to deploy in production enviroment many instances running for obivious reasons

. whenever newer version application build and avilable in docker hub registory

. however you do not  want upgrade all of the at once, this may impact user accessing application

. you might want to upgrade one after another that kind of upgrade known as rolling update

. some rolling update perform unexpected error you ask undo recent change 

. pause and resume the changes in the deployment all options avilable in kubernetes deployment

. each encapsulated with pod	


Deployment definition file
--------------------------

vim deployment-definition.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-deployment
  labels:
    app: myapp
spec:
  selector:
    matchLabels:
      app: myapp
  replicas: 6
  template:
    metadata:
      name: nginx-2
      labels:
        app: myapp
    spec:
      containers:
        - name: nginx 
          image: nginx    



 $ kubectl create -f deployment-definition.yaml

 $ kubectl get deployments

 $ kubectl get pods

view all kubernetes object
--------------------------

$ kubectl get all

-------------------------------------------------------------------------------------------------------------------
27.01.2021
==========

Namespaces
----------

Kubernetes supports multiple virtual clusters backed by the same physical cluster. These virtual clusters are called namespaces


. so farwe created pod, deployment services, whatever we created comes under namspaces

. this namespace known as default namespaces it created automatically when cluster first setup

. kubernetes created set of pods and services for internal purposes those required for networking and dns purposes

. to isolate these from user prevent accidently deleting or modifing services 

. kubernetes create under another namespace at cluster startup named kube-system

. 3rd namespace created automatically called kube-puplic

. you can your namespace 

. create same namespaces for dev and prod but isolate resources between them

. you can create different namespace for each of them

. each of these policies have own set of policies who can do what

. each of namespaces have resource limits

. resources with in namespaces refer simply by their names

. this case we-app-pod simple reach db service using hostname



$ kubectl get pods

. above command list the pods only in the default namespaces

to lsit another namespace pods

$ kubectl get pods --namespace=kube-system


create pod defintion file
------------------------

$ kubectl create -f pod-defintion.yaml

it will create pod on default namespace

to create another namespace use namespace option

$ kubectl create pod -f pod-defintion.yaml --namespace=dev

. alternativly yo can specify the namespace in metadata section  it will ensure pod always created on same name space



how to create namespace
----------------------

. like aother option use namespace defintion file

vim namespace-definition.yaml

apiVersion: v1

kind: Namespace

metadata: 

  name: dev


$ kubectl create -f namespace-definition.yaml

another way

$ kubectl create namespace dev



three namespaces
----------------

  dev                                      default                                         prod

          
$ kubectl get pods --namespace=dev     kubectl get pods                            kubectl get pods --namespace=prod


we want switch dev name space permanently

$ kubectl config set-context $( kubectl config current-context ) --namespace=dev

you should mention other namespace option

view all namespace pods

$ kubectl get pods --all-namespaces

 

limit resource for namespace
----------------------------

vim compute-quota.yaml

apiVersion: v1

kind: ResourceQuota

matadata: 
  name: compute-quota
  namespace: dev

spec:
  hard:
    pods: "10"
    requests.cpu: "4"
    requests.memory: "5Gi"
    limits.cpu: "10"
    limits.memory: 10Gi

$ kubectl create -f compute-quota.yaml

----------------------------------------------------------------------------------------------------------

29.01.2021
=========

kubernetes-services
-------------------

. kubernetes services enable communication between various component within and outside the of the application

. kubernetes help us applications connect together 

. group of pods running with set of services  frontend and backend aonther one external data source

. service enable communication between them

. services enable loose coupling between microservices 


External communication
----------------------

we have deploy we application in the running pod

. node ip address 192.168.1.2 my labtop on same network as well, internal pod network range 10.244.0.0 

. clearly i can't access or ping pod from our labtop to overcome this issue services comes

. kubernetes service is a object just like pod, replicaset, deployment, one of it's use case listen port on the node and forward that request to pod running the webapplication this type of service known us nodeport service

. other kind of services available which we now going to discuss



types of services
-----------------

1. nodeport

. service makes internal pod acccssible to end users it's called nodeport

. there are 3 ports involved the port on the pod actual web server running is 80, it's refered to target port thats where service forward the request

. second port the port on the service itself, it's simply refered to port,  service like virtual server inside the node, cluster it has it's own ip address, it's called cluster ip of the service 

. finally we use port node itself it uses weapplication access externally known as nodeport, it's set to 30008 node port only be valid range

30000 to 32767 

2. clusterip

service create virtual ip inside the cluster enable communication between different services to setof frontend servers to setof backend servers

3. load balancer

this service provide load balancer for our application distribute the load across the different web server in fronend tier



lets now look how to create service we use definition file to create a service

vim service-definition.yaml


apiVersion: v1

kind: Service

matadata:

  name: myapp-service

spec:

  type: NodePort
  ports:
  - targetPort: 80
    port: 80
    nodePort: 30008

 selector:
   app: myapp
   type: fron-end



kubectl apply -f service-defintion.yaml



to view created service

$ kubectl get services

curl http://192.168.1.2:30008





multiple pods
------------

. production enviroment need to run multiple pods for high availability

. they all have same labels with key app and value myapp same label used as a selector match the pod create the pods, it uses random alogrithm to balance the load
.

finally we look pod distribute across multiple node, this case we have wepapplication pods on sepaerate node on the cluster, kubernetes automatically create span across all nodes with same nodeport 30008

--------------------------------------------------------------------------------------------------------

kubernetes-service-cluster-ip
-----------------------------

. group of pods running frontend web server, anotherv set of running backend server and another set of pods for db

. web frontend server need to communicate with backend and backend server need to communicate with redis database etc

. so what is right way to to estabilish connctivity between services

. pods ip assigned to them these ip as known not static, pods goes down and new one created anytime, so we can't rely pod ip for internal

 communication purposes

. kubernetes service group pods together can give single interface to access the pod in the group

. each service have ip name assigned to them that should be used by other pods to access the service , this type of service known as

cluster ip



vim service-definition.yaml

apiVsersion: v1

kind: Service

metadata:

  name: back-end

spec: 
  type: ClusterIp
  ports:
  - targetPort: 80
    port: 80
  selector:
     app: myapp
     type: back-end


$ kubectl apply -f  service-definition.yaml

to view services

$ kubectl get services

-------------------------------------------------------------------------------------------------------

services load-balancer
---------------------

steps
----
1. create Deployments

2. create service (Cluster ip)

3. create service (Load balancer)


application is voting app and result app


. these pods on worker node in the cluster, we have four node cluster 

. service nodeport helps receiving traffic  and routing traffic to respective pods

. but what url to give end user to access the applications, they only want single url to access the applications

. like http://example-voting-app.com  http://example-result-app.com need to achive one way


. create load balancer in the new vm installed suitable load balancer on  it like ha proxy


. then configure loadbalncer to route the traffic to underline node



. if your supported cloud platform like aws azure kubernetes supporting native load balancer integrate with applications

. in service defintion file set type loadbalancer instead of nodeport

--------------------------------------------------------------------------------------------------------------------

01.02.2021
----------

imperative vs declarative in kubernetes
---------------------------------------


imperative
----------

Kubernetes objects can quickly be created, updated, and deleted directly using imperative commands built into the kubectl command-line tool. This document explains how those commands are organized and how to use them to manage live objects


commands
--------

$ kubectl run nginx --image=nginx

$ kubectl create deployment --image=nginx nginx

$ kubectl expose deployment nginx --port 80

$ kubectl edit deployment nginx

$ kubectl scale deployment nginx --replicas=5

$ kubectl set image deployment nginx --image=nginx:1.8


we also use object configuration file to manage object

$ kubectl create -f nginx.yaml


editing object

$ kubectl replace -f nginx.yaml

$ kubectl delete -f nginx


it's difficult to tarck that where object configuration file can help




Declarative
-----------

Declarative creating, updating deleting object using apply command read configuration file what need to done bring infra expected state


$ kubectl apply -f nginx.yaml

apply command will look existing configuration figure out what changes need to made 


exam perspective  you could use imaperative commands to save time

-------------------------------------------------------------------------------------------------------------
04.02.2021
==========

How kubectl apply command works
-------------------------------

Declrative
----------

$ kubectl apply -f nginx.yaml


$ kubectl apply -f /path-to/configfiles




vim nginx.yaml

apiVersion: v1

kind: Pod

metadata:

  name: myapp-pod
  labels: 
    app: myapp
    type: front-end-service

spec: 

  containers:
  - nginx-container
    image: nginx:1.18

   


update objects
-------------

kubectl apply -f nginx.yaml


. apply command take into local file consideration and last applied configuration before make decision what changes to be made 

. when you run apply command if object doesn't exist object is created but additional field store data object 

. yaml version of local configuration file converted to json format then it's stored last applied configuration 

. if you update the object it's comapared loacal, last applied config and live file  match all three files idetify the what change to be made 

. if you update the nginx1.18 to nginx1.19 it's comapred with live if differ version it's update the live file then automatically update the last

 applied configuration file

. then why we need last applied configuration, last applied config help us what field removed from live config file

----------------------------------------------------------------------------------------------------------------------------

04.02.2021
==========

scheduling introduction
-----------------------

1. labels and selectors

2. Resuorce Limits

3. manual scheduling

4. Daemon Sets

5. Multiple schedulers

6. scheduler Events

7. configure Kubernetes scheduler


--------------------------------------------------------------------------------------------------------------------------

11.02.2021
==========

. Lets see simple cluster scenerio 4 nodes, we also have set of pods to deploy node

. when pods created, kubernetes scheduler try to place pod to available worker node

. now no limitations and restrictions to pod place the pod equally

. let us a assume we have dedicated resuorce for node1 for particular use case

. first we prevent node1 to place pod taint on node1 lets call blue

. by defult pod don't have tolerations 

. we must specify which to tolerant to particular pod, we add tolerations pod d

. pod d now tolerant to blue

. rememember taints set to node, tolerations set to pod



Taint the node
--------------

kubectl taint nodes node-name key=value:taint-effect 

. if you like to dedicate pod to the node blue then key app=blue, taint effect define what would happen to pod 

. there are three taint effect 

  1. No schedule

 which means no pod schedule on node1 
  
  2. PreferNoschedule

 which means try to aviod placing pod on node

  3. NoExecute

new pod not schedule on node if existing pod on node


Example command
---------------

$ kubectl taint nodes node1 app=blue:NoSchedule


vim pod-definition.yaml

apiVersion: v1

kind: Pod

metadata:

  name: myapp-pod

spec:
  containers:
  
  - name: nginx-container
    
    image: nginx

  tolerations:
  
  - key: app
    operator: Equal
    value: blue
    effect: NoSchedule

need to be encoded in double quotes


Taint-NoExecute
---------------

. if have two pods on node1 d and c set Noexecute to pod d, it's evoct the pod c

. taints and tolerations only restrict and limiation for nodes not sceduling pods

. taints and tolerations not schedule the pod just tell the node to accept certain tolerations

. if your requirement restrict pod to certain node, we use node affinity 


. we have master node, techically it's just a another node all the hosting pod plus, all the mangement software 

. scheduler doens't schedule any pod on master node why ? kubernetes setup first set taints to master node prevent any pod schedule on the node


$ kubectl describe node kubemaster | grep Taint


----------------------------------------------------------------------------------------------------------------------

Node-selector
============

. We have 3 node cluster with lower resource configuration with one of the them larger configuration

. Different kind of work load running on your cluster, you like to dedicate data process require higher host power, that only node not out of 

resources 

. current defulat setup any pods can goes to any node 


Node selector pod definition file we add new property called node selector on spec section

vim pod-defintion.yaml

apiVersion: v1

kind: Pod

matadata:

  name: myapp-pod

spec:

  containers:
  - name: data-processor
    image: data-processor
  nodeSelector: 
    size: Large


. where did size come from how kubernetes know which one large

. the key value pair, size and large infact label assign to node

. you first label your node prior to create pod


label nodes
-----------

$ kubectl label nodes <node-name> <label-key> <label-value>

$ kubectl label node node-1 size=large


kubectl apply -f pod-defintion.yaml

now pod assigned to node-1


node selector served purpose 

---------------------------------------------------------------------------------------------------------

Node Affinity
=============

This page shows how to assign a Kubernetes Pod to a particular node using Node Affinity in a Kubernetes cluster

 

 You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:

    Katacoda
    Play with Kubernetes

Your Kubernetes server must be at or later than version v1.10. To check the version, enter kubectl version.

--------------------------------------------------------------------------------------------------------------------

17.02.2021
==========

Resource limits
---------------

. lets look at 3 node kubernetes cluster

. each nodes set of cpu, memory and disk resuorces available

. Every pod consume set 0f resources this case 2 cpu one memory and some disk space

. kubernetes scheduler decide which node pod goes to

. scheduler take into consideration resources required by pod

. this case scheduler new pod on node2, node has no sufficent resources, schedluer avoid placing pod on that node, instead place pod on where

 sufficient resiurces available

. in sufficent resuorce available any of the node kubernetes hold back place pod on node, it's shows pending status 


Now focus resource requirement of each pod
------------------------------------------

. by default kubernetes container require 0.5 cpu and 256mi memory to run pod this known us resource requirement for container 

. when scheduler try to place pod on the node these resource identify sufficent number of resources available

. if you know your application need more than this, you can modify this value pod or deployment definition file


vim pod-definition.yaml

apiVersion: v1

kind: Pod

metadata: 

  name: simple-web-color
  labels:

    name: simple-web-color

spec:

  containers:
  - name: simple-web-color
    image: simple-web-color
    ports:
      containerPort: 8080
      
    resources:
      requests:
        memeory: 1Gi
        cpu: 1
      limits:
        memory: 2gi
        cpu: 2


0.1 cpu -> 100m


you can set resource limit for usage pod

by default kubernetes set limit 1vcpu memory 512 mi you don't like default limit, you can set limit und resource section

. resource limit and request each node node on pod defintion file

. pod get exceeds beyonds resource limits, conatiner use more memory contantly it's terminate the pod 

---------------------------------------------------------------------------------------------------------------------------
21.02.2021
----------

DaemonSets
----------

 
. Daemon Sets like replicasets it's help to deploy multiple instances of pod but run one copy of pod on each node 

. Whenever new node added to cluster replica of the pod automattically addded to node, if node removed pod automatically removed 

. Daemon sets ensure one copy of the pod always running on the node


some use cases of daemon sets
----------------------------

. Deploying Monitoring agent or log collector deploying each of the node 

. Deploying monitoring agent in the form of pod all the node in your cluster 

. whether changes in your cluster, daeonsets will take care for you 

. one of the worker node component require every node, it's kube proxy that one use case of daemonsets

. kube-proxy can deploy as a deamonsets 

. another use case for networking 

----------------------------------------------------------------------------------------------------------------------------

27.02.2021
==========

static pods
-----------

. Earlier we talk about kubernetes architecture and how kubelet functions as many control plane component 

. the kubelet relay on kube-api server for instruction what pod load on a node, based decision made by kube scheduler 

. what happened no kube-apiserver, kube-scheduler, etcd cluster, kube-controller

. anything kubelet can do, kubelet captain on the ship, can operate independent on node 

. kubelet can manage the node independely, host we have kubelet installed have docker as well run container

. no kubernetes cluster, there is no kube-api server, one thing kubelet knows to do create pod

. we don't have api sever to provide pod details, we need pod definition file to create pod

. kubelet periodicly check this directory for files and create pod on host, create pod that ensure pod stays alive

. if application restarted kubelet attempt to restart pod

. any change with in this directory kubelete recreate the pod

. if remove file from this directory pod automatically removed , these component created by kubelet without kube-apiserver and rest of kubernetes component

. kubelet only understand pod level 

. what is designated folder it could be any folder on host

. once create pod use docker ps command to check pod

. if kubelet create static pod, its part of cluster, it also create mirror api server, it's just a read only mirror of the pod 


different between staic pod and daemonsets
------------------------------------------

Daemonsets as we saw earlier ensure one instances of applications run all nodes in the cluster it hanled by Daemonset controller throuh kube-apiserver

static pod directly created by kubelet and create rest of kubernetes component

static pod used to create kubernetes control plane component itself, ignore by kube-scheduler

-------------------------------------------------------------------------------------------------------------------------------

27.02.2021
==========

multiple scheduler
-----------------

. you decide to have own scheduling algorithm, so that you can add custom condition 

. kubernetes is highly extensible you can write your own kubernetes scheduler, deployed as a default scheduler 

. kubernetes scheduler can have multiple scheduler 

. kubeadm scheduler deploy the schedluer as a pod

. multiple schedluer one can be active at a time, elect the leader 


view scheduler
-------------

$ kubectl get pods --namespace=kube-system

pod-definition.yaml

apiVersion: v1

kind: Pod

meatadata:

  name: nginx

spec:

  containers:
    image: ngnix
    name: nginx
  schedulerName: my-custom-scheduler


kubectl create -f filename.yaml

view events

kubectl get events


view the log of the pod

$ kubectl logs my-custom-scheduler --name-space=kube-system

---------------------------------------------------------------------------------------------------------

28.02.2021
==========

Logging and monitoring
----------------------

1. Monitor cluster compontent

2. monitor cluster component logs

3. Monitor applications

4. Application logs


monitoring kubernetes cluster
-----------------------------

. how do monitor resource consumption in kubernetes, more importantly what would like to monitor

. like to monitor node level metrics, no of nodes in the cluster, how many of them healthy 

. as well as performance metrics cpu, memory and disk utilization,network 

. pod level metrics performance of each pod cpu and memory consumption on them

. monitoring these data store them and provide analytics on the data

. kubernetes does'nt come fullfilled monitoring solution 

. number of opensource solution available today metric server, prometheus, Elastic stack, datadog, dynatrace

. heapster was one of the original projects monitor and analyis process for kubernetes, heapster now deprecated 

. you can have one metric server on kubernetes cluster, metric server retrive metrics from each of the kubernetes cluster nodes, aggrecate them and store them in memory

. metric server only in memory monitor solution for monitor solution, you can't see historical data performance

. kubernetes run agent on each node kubelet responsible for instruction from api server 

. kubelet also called sub component called cadvisor, cadvisor responsible for receiving performance metrics and exposing tjhem throuh kubelet 


. if you use minikube for your local cluster 

minikube addon enable metrics-server

view node memory consumption
---------------------------

$ kubectl top node

view pod performance metrics
----------------------------

$ kubectl top pod


--------------------------------------------------------------------------------------------------------------------------------------

02.03.2021
==========

Application-life-cycle-management
---------------------------------

. We start at rolling update and roleback configure application, scale application 



 
Rollback in a deployment
------------------------

. Before look how we upgrade our applications understand rollout and versioning in a deployment

. when you first create deployment it triggers rollout, new rollout create new deployment revision call it revision 1

. when application upgraded meaning container version upgrade new one, new deployment revsion created named deployment 2

. this help us keep track changes in our deployment 


Rollout command
---------------

$ kubectl rollout status deployment/myapp-deployment


to see revision and history
---------------------------

$ kubectl rollout history deployment/myapp-deployment



Deployment statrgey
-------------------


. two types of deployment stargeies for example you have 5 replicas of your web appplication deployed 

. first way to destory older vsersion and create newer version application but our application will down in accessible to user this not best way to do it thats recreate statergy

. second way update version one after another it will not affect the application user can access the application thats called rolling update	

. rolling update is a default deployment statergy 


kubectl apply
-------------

$ kubectl apply -f deployment-definition.yaml


$ kubectl set image deployment/myapp-deployment --image=nginx


recreate vs rolling update


$ kubectl describe deployment/myapp-deployment

it scale down 0 pod and scale up 5 pod


rolling upadte scaling down one at time same time up one at time


upgrades
--------

view status of uprades


$ kubectl get replicasets



rollback
--------

$ kubectl rollout undo deployment/myapp-deployment



summarize commands
-----------------

create --> kubectl create -f deployment/myapp-deployment


get ---> kubectl get deployment


update --> kubectl apply -f deployment/myapp-deployment

       ---> kubectl set image deployment/myapp-deployment --image=nginx:1.17


status ---> kubectl rollout status deployment/myapp-deployment

            kubectl rollout history deployment/myapp-deployment

            kubectl rollout undo deployment/myapp-deployment

------------------------------------------------------------------------------------------------------------------------

08.03.2021
==========

Enviroment variable in application
----------------------------------

. how to set enviroment variable in kubernetes

. set enviroment variable  use env property, env is a array  every item in array start with -

. each item has name and value property 


env varibale type
-----------------

1. plain key value format

env

  - name: banana
    value: vitamin

2. configmaps

env:

  - name: APP_COLOR
    valueFrom:
        configMapKeyRef

3. secrets

env:

  -name: APP_COLOR
   valueFrom:
      secretskeyRef
------------------------------------------------------------------------------------------------------------------------

10.03.2021
==========

configuration data in kubernetes
--------------------------------

. config map are used pass configuration data in a key value pairs in kubernetes 

. when pods is created inject the config maps into the pod, key value pair available as a enviroment variable

two ways create config maps
---------------------------

imperative

declerative



kubectl create configmap\
 app-config --from-literal=APP_COLOR=blue 


kubectl create configmap\
 app-config --from-file=app_config.properties


vim config-map.yml
config-map.yml
apiVersion: v1

kind: configMap

metadata: 
  name: app-config

data:

  APP_COLOR: blue
  APP_MODE:  prod

kubectl create -f config-map.yml



envFrom:

  - configMapRef: 

      name: app-config

----------------------------------------------------------------------------------------------------------------------

13.03.2021
==========

Secrets
-------

. Here we have simple python web application lets connect to mysql database

. if you closely look in to code you username and password given in the code

. move this value to config map, configmap store configuration data in plain text format

. scretes comes in store in encrypted and hashed format

. secrets used to store sensitive information like passwords keys they stored encoded and hashed format

two methods
-----------

1. create secrets

2. inject in to pod


imerative way and declerative way


kubectl create secret generic \

 app-secret --from-literal=DB_Host=mysql

            --from-literal=DB_User=root
            --from-literal=DB_Password=paswd  


kubectl create secret generic \

app-secret --from-file=<file-path>


secret-data.yaml

apiVersion: v1

kind: Secret

kind:

  name:app-secret

data:

  DB_Host: mysql
  DB_user: root
  DB_password: passwd

$ kubectl create -f secret-data.yaml 


while we create secret you must specify in hased format  


convert plain text to encoded format

echo -n 'mysql'| base64
bXlzcWw=

echo -n 'root'| base64
cm9vdA==


echo -n 'passwd'| base64
cGFzc3dk


view secrets
------------

$ kubectl get secrets


$ kubectl describe secrets



decode encoded format
---------------------

echo -n 'bXlzcWw='|  base64 --decode

mysql


echo -n 'cm9vdA=='| base64  --decode

root

echo -n 'cGFzc3dk'| base64  --decode

passwd



inject into pod definition file

under spec section 

envFrom:

  secretRef:

        name: app-secret
 

---------------------------------------------------------------------------------------------------

24.03.2021
==========


Cluster upgrade process
------------------------

. in previous lectures we saw how kubernetes manages software releases

. we keep dependencies like ETCD and Core dns now focus core controlplane components 

. it's all of controlplane compontent have same versions

. kube-api server is primary component in control plane is should be higher versions

. control-manager, kube-scheduler,	kubectl can be x- i versions if kube-api version is x  


. kubelet and kube-proxy could be x-2


. kube-controll utility could be higher than kube-api server

. kubernetes support upto three recent minor versions

. upgrade version one minor versions at a time 

. you have cluster master and worker node

. first upgrade your master node then upgrade your worker node 

. if you upgrade master all control plane component goes down but users can access app from worker node

. you can't deploy new app, kubectl not working 

. worker node upgrade one node at a time this way is recomended



command
-------

$ kubeadm upgrade plan


kubeadm not upgrade kubelet you need to upgrade mannually


$ sudo apt-get upgrade -y kubeadm=1.12.0-00

kubeadm upgrade apply v1.12.0


$ kubectl get nodes 

it shows still same version 

upgrade kubelet



$ sudo apt-get upgrade -y kubelet=1.12.0-00


$ systemctl restart kubelet

$ kubectl get nodes


worker node upgrade
-------------------

$ kuebctl drain node01

$ sudo apt-get upgrade -y kubeadm=1.12.0-00

$ sudo apt-get upgrade -y kubelet=1.12.0-00

$ sudo apt-get upgrade node config  --kubelet-version v1.12.0

$ systemctl restart kubelet 

$ kubectl get nodes


$ kubectl uncordon node01

-------------------------------------------------------------------------------------------------------------------






























 












 	







   
 



















     



    



                       









  












     










. 














	




	






 

 
 	 
   
   
  



